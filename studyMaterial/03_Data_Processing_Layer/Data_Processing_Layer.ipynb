{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "    1. Introduction\n",
    "    2. Apache Spark\n",
    "        2.1 Data Science tasks\n",
    "        2.2 Storage layers for Apache Spark\n",
    "        2.3 Spark Context: Role\n",
    "    3. Introduction to the problem\n",
    "        3.1 PageRank\n",
    "        3.2 PageRank Algorithm\n",
    "        3.3 Understandimg dataset\n",
    "    4. Programming with RDDs\n",
    "        4.1 What are RDDs?\n",
    "        4.2 Creating RDDs\n",
    "        4.3 Opertions on RDDs\n",
    "            4.3.1 Transformations\n",
    "            4.3.2 Actions\n",
    "        4.4 Quiz\n",
    "    5. Project\n",
    "        5.1 Introduction: Zipf's Law\n",
    "        5.2 Visualization with NumPy\n",
    "        5.3 Word Count from Project Gutenberg Free ebook Texts\n",
    "        5.4 Tasks\n",
    "        \n",
    "    6. Apache Spark Architecture\n",
    "        6.1 Why to learn underlying architecture\n",
    "        6.2 Spark Runtime architecture\n",
    "        6.3 The Driver\n",
    "        6.4 The Executor\n",
    "        6.5 Cluster manager\n",
    "        6.6 Difference between Job and Tasks\n",
    "        6.7 Summary\n",
    "        6.8 QUIZ\n",
    "        \n",
    "    7. Spark SQL\n",
    "        Part 1 - Dataframes\n",
    "        \n",
    "        7.1 Introduction: Dataframes\n",
    "        7.2 Why Dataframes are useful?\n",
    "        7.3 Create a Dataframe\n",
    "        7.4 Introduction to Dataset\n",
    "        7.5 Selecting Colums\n",
    "        7.6 Operations on Coolumns\n",
    "        7.7 User Defined functions\n",
    "        7.8 Quiz\n",
    "        \n",
    "        Part 2: SQL Queries\n",
    "        \n",
    "        7.9 Register temproary table\n",
    "        7.10 SQL Queries with Spark\n",
    "        \n",
    "        Project\n",
    "        \n",
    "    8. Power of MLlib:Apache Spark\n",
    "        \n",
    "        8.1 Machine Learning with Spark \n",
    "        8.2 Introduction to the problem \n",
    "        8.3 ML Pipelines \n",
    "        8.4 Tasks \n",
    "        8.5 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick up the threads\n",
    "\n",
    "In Big Data Architectures, after Data Ingestion and storage layer, there comes Data Processing layer. In the Data Processing Layer, we will discuss Aache Spark to process Big Data in a distributed and efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "With the rapid growth of emerging applications like social network, semantic web, sensor networks and LBS (Location Based Service) applications, a variety of data to be processed continues to witness a quick increase. Effective management and processing of large-scale data poses an interesting but critical challenge. Once you have Data collected at one layer, next step is to take it to proccessing layer.\n",
    "\n",
    "In this concept we will learn ways to proccess big data in a distributed environment with parallel computation. Tools used in industry at this layer:\n",
    "\n",
    "**Apache Spark:**  Spark is the open standard for flexible in-memory data processing that enables batch, real-time, and advanced analytics on the Apache Hadoop platform.\n",
    "\n",
    "**Apache Flink:** Apache Flink is an open source streaming platform which provides you tremendous capabilities to run real-time data processing pipelines in a fault-tolerant way at a scale of millions of events per second. It has better support for stream processing and some significant improvements.\n",
    "\n",
    "**Apache Spark vs Apache Flink**\n",
    "\n",
    "- Flink and Spark are both general-purpose data processing platforms and top level projects of the Apache Software Foundation (ASF). While Spark is a batch oriented system that operates on chunks of data, called RDDs, Apache Flink is a stream processing system able to process row after row in real time.\n",
    "\n",
    "**Apache Sqoop:** Apache Sqoop is a tool in Hadoop ecosystem which is designed to transfer data between HDFS (Hadoop storage) and relational database servers like mysql, Oracle RDB, SQLite, Teradata, Netezza, Postgres etc. Apache Sqoop imports data from relational databases to HDFS, and exports data from HDFS to relational databases. It efficiently transfers bulk data between Hadoop and external datastores such as enterprise data warehouses, relational databases, etc.\n",
    "\n",
    "**Why Apache Sqoop?**\n",
    "\n",
    "Now, as we know that Apache Flume is a data ingestion tool for unstructured sources, but organisations store their operational data in relational databases. So, there was a need of tool which can import and export data from relational databases. This is why Apache Sqoop was born. Sqoop can easily integrate with Hadoop and dump structured data from relational databases on HDFS, complimenting the power of Hadoop. This is why Apache Flume is an important part of Hadoop Ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "##  What is Apache Spark?\n",
    "\n",
    "**Apache Spark is a cluster computing platform designed to be fast and general-purpose.**\n",
    "\n",
    "- On the speed side, Spark extends the popular MapReduce model to efficiently support more types of computations, including interactive queries and stream processing.\n",
    "\n",
    "- One of the main features Spark offers for speed is the ability to run computations in memory, but the system is also more efficient than MapReduce for complex applications running on disk.\n",
    "\n",
    "- At its core, Spark is a **“computational engine”** that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/apache-spark-vs-hadoop-mapreduce.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Apache Spark: Data Science tasks\n",
    "\n",
    "- Data scientists job includes experience with SQL, statistics, predictive modeling (machine learning), and programming, usually in Python, Matlab, or R. Data scientists also have experience with techniques necessary to transform data into formats that can be analyzed for insights (sometimes referred to as data wrangling)\n",
    "\n",
    "\n",
    "- Spark’s speed and simple APIs can make their life bit easy, and its built-in libraries mean that many algorithms are available out of the box.\n",
    "\n",
    "- The Spark shell makes it easy to do interactive data analysis using Python or Scala.\n",
    "\n",
    "- Spark SQL also has a separate SQL shell that can be used to do data exploration using SQL, or Spark SQL can be used as part of a regular Spark program or in the Spark shell.\n",
    "\n",
    "- Machine learning and data analysis is supported through the MLLib libraries or you can load your own models too.\n",
    "\n",
    "- Spark provides a simple way to parallelize these applications across clusters, and hides the complexity of distributed systems programming, network communication, and fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Storage layers for Spark\n",
    "\n",
    "- Spark can create distributed datasets from any file stored in the Hadoop distributed filesystem (HDFS) or other storage systems supported by the Hadoop APIs. Some examples: **local filesystem, Amazon S3, Cassandra, Hive, HBase, etc.**\n",
    "\n",
    "**Note:** Spark does not require Hadoop; it simply has support for storage systems implementing the Hadoop APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 SparkContext\n",
    "\n",
    "Before starting on writing a spark application it's necessary to understand Spark context. Spark context sets up internal services and establishes a connection to a Spark execution environment. \n",
    "\n",
    "## Role of a Spark Context\n",
    "\n",
    "**Spark Context is the entry point. Like a key to your car.**\n",
    "\n",
    "- Tells Sparks how to access a cluster\n",
    "\n",
    "- Allocates Executors\n",
    "\n",
    "-  The context, living in your driver program, coordinates sets of processes on the cluster to run your application.\n",
    "\n",
    "- The context keeps track of live executors by sending heartbeat messages periodically. \n",
    "\n",
    "\n",
    "- Third, the context may perform dynamic resource allocation if the cluster manager permits. This increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands.\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "In the sense, if you want to compute a complex aggregation on spark, you need to distribute the task in the cluster.\n",
    "\n",
    "Spark context is the gateway to a Spark Cluster.\n",
    "\n",
    "**Example 2:**\n",
    "\n",
    "If you have a dataset ( simple CSV/TXT file) and want computations on this data, you want all the worker nodes to have access to this data. Use the spark context to broadcast this file to all the nodes.\n",
    "\n",
    "- It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The resource manager can be one of these three- Spark Standalone, YARN, Apache Mesos\n",
    "\n",
    "<img src = \"images/sc.jpeg\">\n",
    "\n",
    "\n",
    "\n",
    "If your have Spark running then instance of `SparkContext` will be available to you. \n",
    "Also, you can use `findspark` module to get running SparkContext into your console. Initiating `SparkContext` in that case would be as follows:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "```\n",
    "\n",
    "> you may need to pass `spark_home` path to `init` method call\n",
    "\n",
    "Example:\n",
    "\n",
    "`findspark.init(spark_home='path_to_spark')`\n",
    "\n",
    "Next, type `sc` to check if you have a SparkContext running: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.107:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test to see if you have access to `sc` variable which is `SparkContext` instance\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 Introduction to the problem\n",
    "\n",
    "### 3.1 PageRank\n",
    "\n",
    "We live in a computer era. Internet is part of our everyday lives and information is only a click away. Just open your favorite search engine, like Google, AltaVista, Yahoo, type in the key words, and the search engine will display the pages relevant for your search. But how does a search engine really work?\n",
    "\n",
    "The usefulness of a search engine depends on the relevance of the result set it gives back. There may of course be millions of web pages that include a particular word or phrase; however some of them will be more relevant, popular, or authoritative than others. A user does not have the ability or patience to scan through all pages that contain the given query words. One expects the relevant pages to be displayed within the top 20-30 pages returned by the search engine. Modern search engines employ methods of ranking the results to provide the \"best\" results first that are more elaborate than just plain text ranking.\n",
    "\n",
    "So, our aim is to **Rank the webpages to provide best search results.**\n",
    "\n",
    "For this, we will use Page Rank Algorithm. The idea that Page Rank introduce is that, the importance of any web page can be judged by looking at the pages that link to it. Let's take an example to understand the algorithm better:\n",
    "\n",
    "<img src = \"images/pagerank.jpg\">\n",
    "\n",
    "\n",
    "### 3.2 How the Algorithm Works?\n",
    "\n",
    "The PageRank algorithm outputs a probability distribution that represents the likelihood that a person randomly clicking on web links will arrive at a particular web page. If we run the PageRank program with the input data file and indicate 20 iterations we shall get the following output: \n",
    "\n",
    " ```\n",
    " url_4 has rank: 1.3705281840649928.\n",
    " url_2 has rank: 0.4613200524321036.\n",
    " url_3 has rank: 0.7323900229505396.\n",
    " url_1 has rank: 1.4357617405523626.\n",
    " ```\n",
    "The results clearly indicates that URL_1 has the highest page rank followed by URL_4 and then URL_3 & last URL_2. The algorithm works in the following manner:\n",
    "\n",
    "If a URL (page) is referenced the most by other URLs then its rank increases, because being referenced means that it is important which is the case of URL_1.\n",
    "If an important URL like URL_1 references other URLs like URL_4 this will increase the destination’s ranking.\n",
    "\n",
    "\n",
    "\n",
    "## 3.3 Introduction to the Dataset\n",
    "\n",
    "Throughout the entire concept, we will be using MEDLINE records (abstracts in the life sciences domain). \n",
    "The links represent content-similarity links, i.e., pairs of abstracts that are similar in the words they contain. For example, consider pmid (unique identifier in the MEDLINE collection) [8709207](https://www.ncbi.nlm.nih.gov/pubmed/8709207). See the \"Related Links\" panel on the right hand side of the browser? The data provided above represent instances of graphs defined by such links.\n",
    "\n",
    "The files are tab-delimited adjacency list representations of the link between these medical records and hence their webpages. The first token on each line represents the unique id of the source node, and the rest of the tokens represent the target nodes (i.e., outlinks from the source node). If a node does not have any outlinks, its corresponding line will contain only one token (the source node id).\n",
    "\n",
    "\n",
    "### Let's get started\n",
    "\n",
    "**Treat**: The function implementing the algorithm logic will be provided :) \n",
    "\n",
    "**Task**: Read the dataset into RDDs and implement a simpler version of PageRank in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Programming with RDDs\n",
    "\n",
    "## RDD: Resilient distributed dataset\n",
    "\n",
    "## 4.1 What are RDDs?\n",
    "\n",
    "-  RDDs stands for Resilient Distributed Dataset.\n",
    "\n",
    "- RDD is the **core abstraction** in Apache Spark.\n",
    "(In case you are thinking, What does abstraction mean in programming? then go through this excellent expalnation over stackoverflow - [Abstraction](https://stackoverflow.com/questions/21220155/what-does-abstraction-mean-in-programming)\n",
    "\n",
    "- An RDD is simply a **immutable distributed collection of elements**. \n",
    "\n",
    "- The name captures two important properties:\n",
    "    - **Resilient** means that we must be able to withstand failures and complete an ongoing computation.\n",
    "    - **Distributed** means that we must account for multiple machines having a subset of data. Formally, RDD is a read-only, partitioned collection of records\n",
    "\n",
    "- In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result.\n",
    "\n",
    "- The data inside a spark application is read into the form of RDDs and then Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n",
    "\n",
    "- RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Creating RDDs\n",
    "\n",
    "Spark provides two ways to create RDDs:\n",
    "\n",
    "- Parallelizing a collection in your driver program.\n",
    "- Loading an external dataset\n",
    "\n",
    "### 4.2.1 Parallelizing a collection in your driver program:\n",
    "\n",
    "Create RDDs using parallelize() method on existing iterable or collection in your driver program:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "Number of partitions in your RDD is  4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "\n",
    "#### splitting into 4 slices\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "#### Print out the type of wordsRDD\n",
    "print(type(wordsRDD))\n",
    "\n",
    "#### Print number of partitions in your RDD\n",
    "num_partitions = wordsRDD.getNumPartitions()\n",
    "print(\"Number of partitions in your RDD is  %s\" % (num_partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting parallelized data at one place\n",
    "\n",
    "Next, look into an RDD to check if our data is embedded correctly into it. For this, call `collect()` method on a RDD which will bring data from all the partitions at one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant', 'rat', 'rat', 'cat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the data which is loaded in a RDD is very large and calling a collect operation on such RDD can be an expensive operation in terms of Memory. In those cases, we can call `take()` method to display a part of our RDD and not whole data. Let's call `take` on wordsRDD\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant', 'rat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.take(2)\n",
    "wordsRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "**Create your first RDD**\n",
    "\n",
    "In this task you will create a RDD containing the numbers in range(1,50) into 20 Partitions\n",
    "\n",
    "1. Generate a list `numbers_set` containing numbers in range(1,50)\n",
    "\n",
    "2. Parallelize the list using SparkContext into 20 partitions to create a RDD `numbers_rdd`.\n",
    "\n",
    "3. Print out number of partitions in the RDD\n",
    "\n",
    "3. Print out the first 2 elements in your RDD \n",
    "\n",
    "4. Print out the whole RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a list `numbers_set` containing numbers in range(1,50)\n",
    "numbers_set = range(1,50)\n",
    "\n",
    "#Parallelize the list using SparkContext into 20 partitions to create a RDD `numbers_rdd`.\n",
    "numbers_rdd = sc.parallelize(numbers_set, 20)\n",
    "\n",
    "#Print out number of partitions in the RDD\n",
    "numbers_rdd.getNumPartitions()\n",
    "\n",
    "#Print out the first 2 elements in your RDD \n",
    "numbers_rdd.take(2)\n",
    "\n",
    "# Print out the whole RDD\n",
    "numbers_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Loading an external dataset\n",
    "**Create a Text RDD**\n",
    "Text file RDDs can be created using SparkContext’s textFile method. This method takes an URI for the file (either a local path on the machine, or a hdfs://, s3n://, etc URI) and reads it as a collection of lines. Here is an example invocation:\n",
    "**Two methods:**\n",
    "\n",
    "- `sc.textFile(\"file_path\")`: Creates a RDD with each line as an element. \n",
    "- `sc.wholeTextFiles(\"file_path\")` :  Creates a PairRDD with the key being the file name with a path.\n",
    "\n",
    "\n",
    "**`sc.textFile()` vs `sc.wholeTextFiles()`:**\n",
    "\n",
    "SparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file. It essentialy means that `sc.textFile()` doesn't give back filenames in the RDD.\n",
    "\n",
    "**Let's load an external dataset into a RDD:**\n",
    "\n",
    "We'll load data from a directory containing multiple txt files into one RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1** \n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "1. Load the data residing in a directory by `sc.wholeTextFiles(<directory_path/filepath>\")` into a RDD `med_rdd`. Here, filename is `medline_pager.txt`\n",
    "\n",
    "2. Print out first three elements of the RDD `med_rdd` using `take()` method on the RDD.\n",
    "\n",
    "3. Print out the number of Partitions created\n",
    "\n",
    "4. Perform 1-3 steps using `sc.textFiles()` to read `medline_pager.txt`.\n",
    "\n",
    "NOTE: Note that sc.textFile() doesn't give RDD with filenames while `sc.wholeTextFiles` returned a paired RDD with key as filenames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load using sc.textFile()\n",
    "\n",
    "med_rdd = sc.wholeTextFiles(\"data/medline_pager.txt\")\n",
    "\n",
    "# 2. Print out first three elements\n",
    "\n",
    "med_rdd.take(3)\n",
    "\n",
    "# 3. Print number of partitions\n",
    "\n",
    "med_rdd.getNumPartitions()\n",
    "\n",
    "# 4. Load using sc.wholeTextFiles()\n",
    "\n",
    "med_rdd = sc.textFile(\"data/medline_pager.txt\")\n",
    "\n",
    "# 5. Print out first three elements\n",
    "\n",
    "med_rdd.take(3)\n",
    "\n",
    "# 6. Print number of partitions\n",
    "\n",
    "med_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** For futher operations, we will use RDD returned by `sc.textFile()` by reading a sample dataset `sample_pager.txt` for tutorial reference purpose. Learners are requested to perform all the taska using dataset `medline_pager.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_rdd = sc.textFile('data/sample_pager.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Operations on RDDs\n",
    "\n",
    "From the Spark Programming Guide:\n",
    "\n",
    ">RDDs support two types of operations: **transformations**, which create a new dataset from an existing one, and **actions**, which return a value to the driver program after running a computation on the dataset.\n",
    "\n",
    "### 4.3.1 Transformation on RDDs\n",
    "\n",
    "- Transformations are kind of operations which will transform your RDD data from one form to another. And when you apply this operation on any RDD, you will get a new RDD with transformed data\n",
    "\n",
    "- Operations like map, filter, flatMap are transformations.\n",
    "\n",
    "- **Note:** when you apply the transformation on any RDD it will not perform the operation immediately. It will create a DAG(Directed Acyclic Graph) using the applied operation, source RDD and function used for transformation. And it will keep on building this graph using the references till you apply any action operation on the last lined up RDD. That is why the transformation in Spark are lazy.\n",
    "\n",
    "\n",
    "### 4.3.2 Actions on RDDs\n",
    "- This kind of operation will also give you another RDD but this operation will trigger all the lined up transformation on the base RDD (or in the DAG) and than execute the action operation on the last RDD. \n",
    "- Operations like collect, take, count, first, saveAsTextFile are actions\n",
    "\n",
    "\n",
    "Let's have a look to data in your RDD currently looks like. We will use `take()` method for this:\n",
    "\n",
    "**`take(n)`**:\n",
    "\n",
    "The action take(n) returns n number of elements from RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this RDD contains on each line, first element as page name and rest list of all neighbors and everything tab seperated.\n",
    "\n",
    "**Generate a web system as an RDD as page name, neighbor page name from the current RDD (page name, list of all neighbors).**\n",
    "\n",
    "- For this, we have to call transformations on our current RDD. First let's create a simple list out of each line removing tabs.\n",
    "\n",
    "**`map():`**\n",
    "The map() transformation takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD.\n",
    "\n",
    "\n",
    "**`filter():`**\n",
    "The filter() transformation takes in a function and returns an RDD that only has elements that pass the filter() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'url_1', u'url_4'],\n",
       " [u'url_2', u'url_1'],\n",
       " [u'url_3', u'url_2', u'url_1'],\n",
       " [u'url_4', u'url_3', u'url_1']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_rdd = url_rdd.map(lambda line: line.split('\\t'))\n",
    "url_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "- Apply map() transformation on RDD created by reading medline_pager.txt dataset. This transformation should result in a RDD `med_rdd` where each web group link is a list. Use map to split each webgrouplink by tab.\n",
    "\n",
    "- Print out the first ten elements of your rdd.\n",
    "\n",
    "**Hint:** \n",
    "\n",
    "- Like shown in example use map(lambda line:line.split('\\t')) on the RDD\n",
    "- Use `take(n)` to print first n elements of the RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's call a customize function to get the RDD in the required format. Note here we want to produce multiple output elements for each input element. The operation to do this is called **flatMap()**.\n",
    "\n",
    "**flatMap():** Similar to map, it returns a new RDD by applying a function to each element of the RDD, but output is flattened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2. Similarly use flatMap on the transformed RDD to ungroup the pagelinks where each line contains a list [main_page, neighbor_page] into a resultant RDD `url_links_rdd`.\n",
    "Use the readymade function `ungroup_weblinks()` to ungroup each pagelinks list.\n",
    "\n",
    "Hint: Use flatMap like this to apply `ungroup_weblinks` : \n",
    "`url_links_rdd = url_rdd.flatMap(lambda line: ungroup_weblinks(line))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'url_1', u'url_4'],\n",
       " [u'url_2', u'url_1'],\n",
       " [u'url_3', u'url_2'],\n",
       " [u'url_3', u'url_1'],\n",
       " [u'url_4', u'url_3'],\n",
       " [u'url_4', u'url_1']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ungroup_weblinks(each_pager):\n",
    "    '''\n",
    "    This function takes up a list of page links in the form [main_page neighbor1 neighbor2 .. neighbor n] and\n",
    "    returns a `list of lists` with ungroup pagelinks lists like \n",
    "    [main_page neighbor1], [main_page, neighbor2],.., [main_page, neighbor n].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    each_pager: list\n",
    "    A list of pagelinks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    final_list: list\n",
    "    List of lists containing ungroup page links.\n",
    "    '''\n",
    "\n",
    "    page_main = each_pager[0]\n",
    "    final_list = []\n",
    "    for i in range(len(each_pager)-1):\n",
    "        final_list.append([page_main, each_pager[i+1]])\n",
    "    return final_list\n",
    "\n",
    "\n",
    "url_links_rdd = url_rdd.flatMap(lambda line: ungroup_weblinks(line))\n",
    "\n",
    "url_links_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, you'll see the ouput is flattened as expected.\n",
    "\n",
    "Do the same operation on `med_rdd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our workflow to solve this problem will be like as shown below:**\n",
    "\n",
    "\n",
    "<img src = \"images/pgalgo.jpg\">\n",
    "\n",
    "**Let's start with creating the links RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Create url link Paired RDD\n",
    "\n",
    "#### PairedRDDs: Working with Key/Value pairs\n",
    "\n",
    "\n",
    "**Paired RDD**: Pair RDD is just a way of referring to an RDD containing key/value pairs, i.e. tuples of data.\n",
    "\n",
    "- Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network. For example, pair RDDs have a reduceByKey() method that can aggregate data separately for each key, and a join() method that can merge two RDDs together by grouping elements with the same key. \n",
    "\n",
    "- We will create a paired RDD from our existing RDD in the next steps\n",
    "\n",
    "**Now, Generate the websystem as an RDD of tuples (page, list of neighbors) from the `url_links_rdd` i.e create a PairedRDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides us with a direct method to group the elements in a RDD by using `groupByKey()/reduceByKey()` method.\n",
    "\n",
    "**groupByKey()**:  We can apply the “groupByKey” / “reduceByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    return urls[0], urls[1]\n",
    "\n",
    "\n",
    "url_links_rdd = url_links_rdd.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "# pg.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check if we got the expected RDD!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'url_3', [u'url_1', u'url_2']),\n",
       " (u'url_1', [u'url_4']),\n",
       " (u'url_4', [u'url_3', u'url_1']),\n",
       " (u'url_2', [u'url_1'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url_links_rdd now contains tuples (page, list of neighbors)\n",
    "url_links_rdd.mapValues(list).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Using the steps shown, Generate the websystem as an RDD of tuples (page, list of neighbors) from the url_links_rdd i.e create a PairedRDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `cache()` method above. **Can you tell why?**\n",
    "\n",
    "Hint - Links RDD will be called multiple times in our solution\n",
    "\n",
    "Answer - When you run a spark transformation via an action (count, print, foreach), then, and only then is your graph being materialized and in your case the file is being consumed. RDD.cache purpose it to make sure that the result of sc.textFile(\"data.txt\") is available in memory and isn't needed to be read over again. In our case we want transformed url link rdd to cached.\n",
    "\n",
    "When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations are again grouped into two types:**\n",
    "\n",
    "- **Narrow transformation:** RDD operations like map, union, filter can operate on a single partition and map the data of that partition to resulting single partition. These kind of operations which maps data from one to one partition are referred as Narrow operations. Narrow operations doesn’t required to distribute the data across the partitions.\n",
    "\n",
    "\n",
    "- **Wide transformation** -RDD operations like groupByKey, distinct, join may require to map the data across the partitions in new RDD. These kind of operations which maps data from one to many partitions are referred as Wide operations. Narrow operations doesn’t required to distribute the data across the partitions. In most of the cases Wide operations distribute the data across the partitions. These considered to be more costly than narrow operations due to data shuffling.\n",
    "\n",
    "<img src= \"images/transformations.jpg\">\n",
    "\n",
    "**Note: Wide transformations results in stages**\n",
    "\n",
    "\n",
    "Data materialization occurs in a few places. When reading, shuffling, or passing data to an action. This is where the distinction between **narrow and wide dependencies** comes up. \n",
    "Narrow dependencies allow pipelining, a state of flow is straightforward. While wide dependencies forbid pipelining by requiring a shuffle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Populating the Ranks Data - Initial Seeds\n",
    "\n",
    "The code below creates \"ranks0\" - a key/value pair RDD by taking the key (URL) from the links RDD and assigning the value = 1.0 to it. Ranks0 is the initial ranks RDD and it is populated with the seed number 1.0 (please see diagram below). In the 3rd part of the program we shall see how this ranks RDD is recalculated at each iteration and eventually converges, after 20 iterations, into the PageRank probability scores mentioned previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = url_links_rdd.map(lambda url_neighbors: (url_neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'url_3', 1.0), (u'url_1', 1.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Looping and Calculating Contributions & Recalculating Ranks\n",
    "\n",
    "\n",
    "This part is the heart of the PageRank algorithm. In each iteration, the contributions are calculated and the ranks are recalculated based on those contributions. The algorithm has 4 steps: \n",
    "\n",
    "1- Start the algorithm with each page at rank 1 \n",
    "\n",
    "2- Calculate URL contribution: contrib = rank/size \n",
    "\n",
    "3- Set each URL new rank = 0.15 + 0.85 x contrib \n",
    "\n",
    "4- Iterate to step 2 with the new rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'url_3', (<pyspark.resultiterable.ResultIterable at 0x7ff111bccb10>, 1.0)),\n",
       " (u'url_2', (<pyspark.resultiterable.ResultIterable at 0x7ff111b7b550>, 1.0)),\n",
       " (u'url_1', (<pyspark.resultiterable.ResultIterable at 0x7ff111b7b410>, 1.0)),\n",
       " (u'url_4', (<pyspark.resultiterable.ResultIterable at 0x7ff111b7bd10>, 1.0))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_links_rdd.join(ranks).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "no_of_iteration = 20\n",
    "for iteration in range(no_of_iteration):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = url_links_rdd.join(ranks).flatMap(\n",
    "        lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_4 has rank: 1.37052818406.\n",
      "url_3 has rank: 0.732390022951.\n",
      "url_2 has rank: 0.461320052432.\n",
      "url_1 has rank: 1.43576174055.\n"
     ]
    }
   ],
   "source": [
    "for (link, rank) in ranks.collect():\n",
    "    print(\"%s has rank: %s.\" % (link, rank))\n",
    "    \n",
    "#The higher the rank, more relevant is the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = ranks.collect()\n",
    "import matplotlib.pyplot as plt\n",
    "x, y = zip(*data)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding the operations we performed:\n",
    "\n",
    "<img src = \"images/pagerank_conclusion.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 QUIZ\n",
    "\n",
    "**Choose the correct answer from the given options for each question**\n",
    "\n",
    "**Q.1 How can I modify in-place every item of the dataset?**\n",
    "\n",
    "A. By invoking a 'flatMap' transformation\n",
    "\n",
    "B. By invoking a 'map' transformation\n",
    "\n",
    "C. You cannot modify data in-place in Spark \n",
    "\n",
    "\n",
    "Answer: **C**\n",
    "Reason: True. Datasets are immutable in Spark, and you cannot modify data in-place.\n",
    "\n",
    "**Q.2 What is a keyed RDD?** \n",
    "\n",
    "A. An RDD with a key.\n",
    "\n",
    "B. An RDD of key-value pairs. \n",
    "\n",
    "C. An RDD that you can use as a key in associative data structures.\n",
    "\n",
    "\n",
    "Answer: **B**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Introduction: Zipf's Law\n",
    "\n",
    "Named after the American linguist George Kingsley Zipf (1902-1950), Zipf's Law describes an empirical law that describes the phenomenon in the physical and social sciences where many types of data can be approximated by Zipf distribution - a family of discrete power law probability distributions.\n",
    "\n",
    "In linguistics, Zipf's law states that given some text of natural language, the frequency of any word is inversely proportional to its rank in the frequency table.  The most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. \n",
    "\n",
    "**For example**,\n",
    "In one sample of words in the English language, the most frequently occurring word, **\"the\"**, accounts for nearly 7% of all the words (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place word **\"of\"** accounts for slightly over 3.5% of words (36,411 occurrences), followed by **\"and\"** (28,852). \n",
    "\n",
    "#### Formula:\n",
    "\n",
    "Zipf's law then predicts that frequency of element of rank $k$ can be approximated by,\n",
    "\n",
    "$$f(k; s, N) = \\frac{1/k^s}{\\sum_{n=1}^N (1/n^s)}$$\n",
    "\n",
    "where $N$ is the number of elements, and $s$ is the value of the exponent characterizing the distribution.  Normally, $s$ is defined to be $1$.\n",
    "\n",
    "To see illustration of Wordcount example visit: http://www.wordcount.org/main.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Visualization with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda rank,elements,s: 1/(np.sum(1./(np.arange(1,elements+1)**s)) * rank**s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranks = np.arange(1,51)\n",
    "N = 5000\n",
    "s = 1.0\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ranks,f(ranks,N,s), 'o-')\n",
    "plt.title(\"Zipfian Distribution PMF\")\n",
    "plt.xlabel(\"k: rank\")\n",
    "plt.ylabel(\"p: probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3  Word Count from Project Gutenberg Free ebook Texts\n",
    "\n",
    "This exercise will attempt to replicate the results from wordcount.org by processing a large volume of sample texts.  We'll use Spark to read the text data, process the texts, and examine the total number of unique words.  \n",
    "\n",
    "We will sample 16 texts from [Project Gutenberg](https://www.gutenberg.org/ \"Title\").\n",
    "Sample text files are in the folder 'books'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Text Files\n",
    "Next, we'll load up the text files and create a text file RDD. Spark provides textFile method to read a text file and return it as a RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 TASKS: \n",
    "**TASK 1.** Read the text files residing into a folder `books` into one single RDD `textsRDD`.\n",
    "Hint: Use `sc.textFile(\"folder_path\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_path = os.path.join('books')\n",
    "textsRDD = sc.textFile(books_path + '/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2.** Clean the data in RDD, by removing all non-alpanumeric characters.\n",
    "Use the readymade `parsewords()` function for cleaning the data.\n",
    "\n",
    "**Hint:** Use `map()` or `flatMap()` on the `textsRDD` to apply `parsewords()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up the words\n",
    "import re\n",
    "\n",
    "def parsewords(sentence):\n",
    "    # Convert all non-alphanumeric characters into empty string\n",
    "    sentence_clean = re.sub(r'([^A-Za-z0-9\\s+])', '', sentence)  \n",
    "    words = sentence_clean.split(' ')\n",
    "    # Convert to lowercase and eliminate empty string words\n",
    "    return [word.lower() for word in words if word != ''] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a a RDD that is a collection of strings. We'll start by mapping the function parsewords to each of the line of strings in textsRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'project',\n",
       "  u'gutenbergs',\n",
       "  u'frankenstein',\n",
       "  u'by',\n",
       "  u'mary',\n",
       "  u'wollstonecraft',\n",
       "  u'godwin',\n",
       "  u'shelley'],\n",
       " [],\n",
       " [u'this',\n",
       "  u'ebook',\n",
       "  u'is',\n",
       "  u'for',\n",
       "  u'the',\n",
       "  u'use',\n",
       "  u'of',\n",
       "  u'anyone',\n",
       "  u'anywhere',\n",
       "  u'at',\n",
       "  u'no',\n",
       "  u'cost',\n",
       "  u'and',\n",
       "  u'with'],\n",
       " [u'almost',\n",
       "  u'no',\n",
       "  u'restrictions',\n",
       "  u'whatsoever',\n",
       "  u'you',\n",
       "  u'may',\n",
       "  u'copy',\n",
       "  u'it',\n",
       "  u'give',\n",
       "  u'it',\n",
       "  u'away',\n",
       "  u'or'],\n",
       " [u'reuse',\n",
       "  u'it',\n",
       "  u'under',\n",
       "  u'the',\n",
       "  u'terms',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'project',\n",
       "  u'gutenberg',\n",
       "  u'license',\n",
       "  u'included']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsRDD.map(parsewords).take(5)  # Not quite what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've used `map()` then the output will be a list of list and this is not something which we want. We want to generate one single list containing all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the result of mapping parsewords to textsRDD did not yield the result that we want.\n",
    "\n",
    "RDD has flatMap method that will map a function that has been passed in as a parameter to elements of the RDD and flatten the result. We'll use flatMap method and apply parsewords function to create a new RDD of words.\n",
    "\n",
    "**TASK 3.** Apply `flatMap` using `parsewords()` to the `textRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'project',\n",
       " u'gutenbergs',\n",
       " u'frankenstein',\n",
       " u'by',\n",
       " u'mary',\n",
       " u'wollstonecraft',\n",
       " u'godwin',\n",
       " u'shelley',\n",
       " u'this',\n",
       " u'ebook',\n",
       " u'is',\n",
       " u'for',\n",
       " u'the',\n",
       " u'use',\n",
       " u'of',\n",
       " u'anyone',\n",
       " u'anywhere',\n",
       " u'at',\n",
       " u'no',\n",
       " u'cost']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRDD = textsRDD.flatMap(parsewords)\n",
    "wordRDD.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `map` method of the RDD to convert each occurance of word into a (key,value) pair and create a new RDD called `wordPairRDD`.  If you saw the word count example in MapReduce, you should be familiar with this pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an RDD of words, let's examine how many words are in our sample texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4.** Find total number of words in the `textRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Word Count:', 2018318)\n"
     ]
    }
   ],
   "source": [
    "# Total Number of Words in our Sample\n",
    "total_count = wordRDD.count()\n",
    "print(\"Total Word Count:\", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5:** Find frequency of each word and give out a RDD in form (word,frequency)\n",
    "\n",
    "Hint:\n",
    "1. Create a PairedRDD `wordPairRDD` out of `textRDD` which contains each individual word into a pair of (word,1) tuple .\n",
    "2. Call `reduceByKey()` on `wordPairRDD` to obtain frequency count of each word in a resultant RDD `wordCountRDD`. This will also result in a RDD containing only unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'project', 1),\n",
       " (u'gutenbergs', 1),\n",
       " (u'frankenstein', 1),\n",
       " (u'by', 1),\n",
       " (u'mary', 1),\n",
       " (u'wollstonecraft', 1),\n",
       " (u'godwin', 1),\n",
       " (u'shelley', 1),\n",
       " (u'this', 1),\n",
       " (u'ebook', 1),\n",
       " (u'is', 1),\n",
       " (u'for', 1),\n",
       " (u'the', 1),\n",
       " (u'use', 1),\n",
       " (u'of', 1),\n",
       " (u'anyone', 1),\n",
       " (u'anywhere', 1),\n",
       " (u'at', 1),\n",
       " (u'no', 1),\n",
       " (u'cost', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordPairRDD = wordRDD.map(lambda word: (word,1))\n",
    "wordPairRDD.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted wordRDD into a a new RDD that holds each instance of word into a (key,value) pair.  Let's use reduceByKey method to sum up the counts and create a new RDD, `wordCountRDD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'aided', 11),\n",
       " (u'voluble', 3),\n",
       " (u'promenade', 1),\n",
       " (u'disgracewhen', 1),\n",
       " (u'fawn', 3),\n",
       " (u'ridden', 2),\n",
       " (u'nun', 5),\n",
       " (u'applyed', 11),\n",
       " (u'joshua', 30),\n",
       " (u'needlessly', 4),\n",
       " (u'four', 392),\n",
       " (u'individualhe', 1),\n",
       " (u'verses', 43),\n",
       " (u'wormlike', 1),\n",
       " (u'120', 8),\n",
       " (u'expressiondown', 1),\n",
       " (u'callianira', 1),\n",
       " (u'relationships', 2),\n",
       " (u'increase', 46),\n",
       " (u'scufflings', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountRDD = wordPairRDD.reduceByKey(lambda x,y: x+y)\n",
    "wordCountRDD.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5:** Calculate total number of Unique words in the dataset\n",
    "\n",
    "Hint: After calling `reduceByKey()`, the resultant RDD conatins only set of unique words so simply call `count()` on `wordcountRDD()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Unique Words:', 56238)\n"
     ]
    }
   ],
   "source": [
    "# Number of Unique Words\n",
    "unique_count = wordCountRDD.count()\n",
    "print(\"Total Unique Words:\", unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 6:** Print out the top 50 most occuring words in the dataset i.e. the top 50 words having highest frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:**\n",
    "\n",
    "Since wordCountRDD contains unique words and their counts, we sort them by takeOrdered method of RDD specifying the the ordering function to order by the values in (key,value) pairs. We will only select top 50 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 112921),\n",
       " (u'and', 70565),\n",
       " (u'of', 60234),\n",
       " (u'to', 55424),\n",
       " (u'a', 39794),\n",
       " (u'i', 33960),\n",
       " (u'in', 33750),\n",
       " (u'that', 27203),\n",
       " (u'it', 25073),\n",
       " (u'was', 22650),\n",
       " (u'he', 22172),\n",
       " (u'his', 20604),\n",
       " (u'with', 17034),\n",
       " (u'as', 16683),\n",
       " (u'is', 15833),\n",
       " (u'for', 15827),\n",
       " (u'you', 15700),\n",
       " (u'but', 14447),\n",
       " (u'not', 14256),\n",
       " (u'be', 13380),\n",
       " (u'had', 13236),\n",
       " (u'her', 12093),\n",
       " (u'at', 11588),\n",
       " (u'my', 11345),\n",
       " (u'on', 11230),\n",
       " (u'by', 10891),\n",
       " (u'all', 10465),\n",
       " (u'have', 9821),\n",
       " (u'him', 9803),\n",
       " (u'she', 9718),\n",
       " (u'me', 9365),\n",
       " (u'so', 9334),\n",
       " (u'this', 9273),\n",
       " (u'from', 8804),\n",
       " (u'or', 8600),\n",
       " (u'they', 8569),\n",
       " (u'which', 8557),\n",
       " (u'no', 7026),\n",
       " (u'there', 7007),\n",
       " (u'we', 6759),\n",
       " (u'were', 6698),\n",
       " (u'said', 6646),\n",
       " (u'when', 6451),\n",
       " (u'if', 6177),\n",
       " (u'one', 6137),\n",
       " (u'their', 6088),\n",
       " (u'are', 5984),\n",
       " (u'them', 5749),\n",
       " (u'what', 5687),\n",
       " (u'would', 5246)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Top 50 words in descending order of frequency\n",
    "top50Words = wordCountRDD.takeOrdered(50, key=lambda x: -x[1]) # Use '-' to sort in descending order\n",
    "top50Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "Although we have performed each transformation in separate steps, Spark enables us to perform these four steps in one line of code by chaining them together.\n",
    "\n",
    "Although Python is strict about indentation, we use a convenient trick of wrapping a chain of RDD methods inside a parenthesis. This trick enables us to chain multiple RDD methods in several lines of code, which enhances readability of your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50Words = (textsRDD.flatMap(parsewords)\n",
    "                      .map(lambda word: (word,1))\n",
    "                      .reduceByKey(lambda x,y: x+y)\n",
    "                      .takeOrdered(50, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot the top50Words rank-frequency count.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = list()\n",
    "counts = list()\n",
    "for pair in top50Words:\n",
    "    words.append(pair[0])\n",
    "    counts.append(pair[1])\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(range(1,51), np.array(counts) / float(total_count), label=\"Word Frequency\", align='center')\n",
    "plt.plot(ranks, f(ranks,N,s), c='r', label=\"$f(k,s,N)$\")\n",
    "plt.xticks(range(1,51), words[:50], rotation=60, fontsize=14)\n",
    "plt.title(\"Frequency of Top 50 Words\",fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0.5,50.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not exact, the word frequency from our sample of texts follows the Zipf distribution closely. We also see that in our sample texts, \"and\" is the second most frequently occuring word. This is in contrast to the word frequency ordering of BNC in which \"of\" is ranked second and \"and\" is ranked third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Spark Runtime Architecture\n",
    "\n",
    "## 6.1 Why to learn underlying architecture?\n",
    "\n",
    "\n",
    "- In previously covered concepts we saw how to run spark application in a standalone mode. When it comes to processing Big Data, the Spark application must be written in a way to optimize the resources allocated to the cluster. The architecture understanding will help in visualizing the parallel processing that occurs inside a spark application.\n",
    "\n",
    "- Having a complete picture is important for you when debugging the application or searching for a documentation. \n",
    "\n",
    "- A delightful treat for all the developers is that writing applications for parallel cluster execution use the same API that on a standalone mode. Means, you can use same `pyspark` script on a standalone mode and cluster mode.\n",
    "\n",
    "\n",
    "## 6.2 Spark Runtime Architecture\n",
    "\n",
    "\n",
    "- Spark can run on a wide variety of cluster managers (Hadoop YARN, Apache Mesos, and Spark’s own built-in Standalone cluster manager) in both on-premise and cloud deployments.\n",
    "\n",
    "- Spark uses a **Master-Slave architecture** in its cluster mode.\n",
    "\n",
    "<img src=\"images/masterSlave.jpeg\">\n",
    "\n",
    "\n",
    "- Single Master and multiple Slaves.\n",
    "\n",
    "- A Spark application is launched on a set of machines using an external service called a **cluster manager**.\n",
    "\n",
    "- A distributed application is placed in execution by a master using a Central coordinator called **Driver**.\n",
    "\n",
    "- Tasks are the smallest unit of work in Spark. One Spark job is divided into multiple tasks.\n",
    "\n",
    "- Executors on worker nodes are responsible for executing these tasks.\n",
    "\n",
    "<img src = \"images/sparkapp.jpg\">\n",
    "\n",
    "Let's get into more details one by one:\n",
    "\n",
    "## 6.3 The Driver\n",
    "    - Runs the main () function of the application and is the place where the Spark Context is created.\n",
    "\n",
    "    - It has two main duties:\n",
    "\n",
    "        - **Converts User Application into tasks**\n",
    "\n",
    "            - Translates the RDD’s into the execution graph and splits the graph into multiple stages\n",
    "            - A Spark program implicitly creates a logical directed acyclic graph (DAG) of operations. When the driver runs, it converts this logical graph into a physical execution plan.\n",
    "        - **Scheduling tasks on executors**\n",
    "            - Given a physical execution plan, a Spark driver must coordinate the scheduling of individual tasks on executors.\n",
    "            - When executors are started they register themselves with the driver, so it has a complete view of the application’s executors at all times.\n",
    "        - **Exposes the information about the running spark application through a Web UI at port 4040**\n",
    "  \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How important is it to have a smart driver?**\n",
    "\n",
    "<img src = \"images/driver.jpg\">\n",
    "\n",
    "The above picture explains enough! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Executors\n",
    "    - Spark executors are worker processes responsible for running the individual tasks in a given Spark job\n",
    "    - Executors are launched once at the beginning of a Spark application and typically run for the entire lifetime of an application, though Spark applications can continue if executors fail. \n",
    "    - Executors have two roles. First, they run the tasks that make up the application and return results to the driver. \n",
    "    - Second, they provide in-memory storage for RDDs that are cached by user programs, through a service called the Block Manager that lives within each executor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Cluster Manager\n",
    "\n",
    "    - Spark depends on a cluster manager to launch executors and, in certain cases, to launch the driver.\n",
    "    - The cluster manager is a pluggable component in Spark. This allows Spark to run on top of different external managers, such as YARN and Mesos, as well as its built-in Stand‐alone cluster manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How these all are chained together?**\n",
    "\n",
    "- Whenever you invoke an action, the job gets spawned in the driver program. \n",
    "- Then the driver runs a job scheduler to divide the job into smaller stages. \n",
    "- tasks are created for every job stage. \n",
    "- Finally, tasks are delegated to the executors, which perform the actual work. \n",
    "- All this machinery exists within the SparkContext object. \n",
    "- **It keeps track of the executors, it spawns jobs, and it runs the scheduler. **\n",
    "\n",
    "## 6.6 What is the difference between job stages and tasks?.\n",
    "\n",
    "- job stages are defined on RDD level. That is, they are not immediately executable. \n",
    "- Tasks, on the other hand, are bound to a particular partitions and thus are immediately executable.\n",
    "- The idea behind the job stages is to pipeline computation as much as possible, avoiding the unnecessary data materializations. For example, if you applied two filter transformations in a row, it is not necessary to serialize and deserialize data in between. You can simply pass the data through the next predicate. \n",
    "\n",
    "\n",
    "## 6.7 Summary\n",
    "**Let's summarize, what happens when an action is called?** \n",
    "\n",
    "The SparkContext is the core of your application. It allows your application to connect to a cluster and allocate resources and executors. Then whenever you invoke an action, the SparkContext spawns a job and runs the job scheduler to divide it into stages. \n",
    "That is pipelineable parts of your computations. And only transformations with narrow dependencies are pipelined. \n",
    "Then tasks are created for every job stage and scheduled to the executors. \n",
    "The driver communicates directly with the executors, exchanging with task information and liveness status. I hope now we have a clear picture of what happens when you invoke an action in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** A spark application consists of 1 Master and may have multiple workers running. There can be multiple executors running on each worker. Also, each executor may run multiple tasks at a time depending on the resources allocated to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 QUIZ\n",
    "\n",
    "Choose the correct answer from the given options for each question\n",
    "\n",
    "**Q.1.** What is a job?\n",
    "A. A pipelineable part of the computation.\n",
    "\n",
    "B. An activity spawned in the response to a Spark action.\n",
    "\n",
    "C. That is how Spark calls my application.\n",
    "\n",
    "D. A dependency graph for the RDDs.\n",
    "\n",
    "E. A unit of work performed by the executor.\n",
    "\n",
    "F. An activity you get paid for.\n",
    "\n",
    "**Answer**\n",
    "B. An activity spawned in the response to a Spark action.\n",
    "\n",
    "**Q.2.** What is a task?\n",
    "\n",
    "A. That is how Spark calls my application.\n",
    "\n",
    "B. An activity spawned in the response to a Spark action.\n",
    "\n",
    "C. A pipelineable part of the computation.\n",
    "\n",
    "D. A dependency graph for the RDDs.\n",
    "\n",
    "E. A unit of work performed by the executor.\n",
    "\n",
    "**Answer**\n",
    "E. A unit of work performed by the executor.\n",
    "\n",
    "**Q.3.** Where are actions executed?\n",
    "\n",
    "A. Driver\n",
    "B. Workers\n",
    "\n",
    "**Answer:** \n",
    "A. Driver\n",
    "\n",
    "**Q.4** How does your application find out the executors to work with?\n",
    "\n",
    "A. You statically define them in the configuration file.\n",
    "\n",
    "B. The SparkContext object queries a discovery service to find them out.\n",
    "\n",
    "C. The SparkContext object allocates the executors by communicating with the cluster manager.\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "C. The SparkContext object allocates the executors by communicating with the cluster manager.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Spark SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spark SQL** - Spark’s interface for working with structured and semistructured data. \n",
    "\n",
    "- **Structured data** is any data that has a schema—that is, a known set of fields for each record.\n",
    "\n",
    "- Spark SQL lets you query structured data inside Spark programs, using either **SQL** or a familiar **DataFrame API**. Usable in Java, Scala, Python and R.\n",
    "\n",
    "- Spark SQL is use to execute SQL queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Apache Spark, **a DataFrame is a distributed collection of rows under named columns.**\n",
    "\n",
    "- In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:\n",
    "\n",
    "    - **Immutable in nature :** We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD  after applying transformations.\n",
    "    - **Lazy Evaluations:** Which means that a task is not executed until an action is performed.\n",
    "\n",
    "    - **Distributed:** RDD and DataFrame both are distributed in nature.\n",
    "\n",
    "- When running SQL from within another programming language the results will be returned as a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DataFrames are Useful ?\n",
    "\n",
    "After learning about pandas dataframes, you must be aware of many advantages that Dataframes provides us with. But the question is, what additional advantages Dataframes in spark provides us with?\n",
    "\n",
    "- DataFrames are designed for processing large collection of structured or semi-structured data.\n",
    "\n",
    "- Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
    "\n",
    "- DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
    "\n",
    "- DataFrame has a support for wide range of data format and sources.\n",
    "\n",
    "- It has API support for different languages like Python, R, Scala, Java.\n",
    "\n",
    "- Like our RDDs are distibuted across machines in a cluster similarly dataframes provides us with distributed computation capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a general rule of thumb, one should consider an alternative to Pandas whenever the data set has more than 10,000,000 rows which, depending on the number of columns and data types, translates to about 5-10 GB of memory usage. At that point PySpark might be an option for you that does the job**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if SparkContext is running!**\n",
    "\n",
    "**Recall - Why do we need a SparkContext running?**\n",
    "\n",
    "- First step, in any Apache programming is to create a SparkContext. SparkContext is required when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.104:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a **SQLContext**. Now, Why **SQLContext**?\n",
    "\n",
    "- The entry point into all relational functionality in Spark is the SQLContext class.\n",
    "- Basically it is must to have SQLContext in order to perform SQL related operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into spark versions<2.0\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into spark version>2.0\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark dataframe basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: `getOrCreate()`-** Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a DataFrame ?\n",
    "A DataFrame in Apache Spark can be created in multiple ways:\n",
    "\n",
    "It can be created using different data formats. For example:\n",
    "1. Loading data from Existing RDD.\n",
    "2. Loading the data from JSON, CSV.\n",
    "\n",
    "### 1.  Creating DataFrame from RDD\n",
    "\n",
    "One can easily create a dataframe out of a List of tuples. Steps can be as follows:\n",
    "\n",
    "1. Create a list of tuples. Each tuple contains name of a person with age.\n",
    "2. Create a RDD from the list above.\n",
    "3. Convert each tuple to a row.\n",
    "4. Create a DataFrame by applying createDataFrame on RDD with the help of sqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [('Sam',25, 'M'),('Jalfaizy',22, 'F'),('Tom',20, 'M'),('Nicky',26, 'F'),('Wrick', 30, 'M')]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1]), Gender=x[2]))\n",
    "schemaPeople = sqlContext.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(schemaPeople)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the DataFrame from external file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Dataset\n",
    "\n",
    "### Context\n",
    "\n",
    "H-1B visas are a category of employment-based, non-immigrant visas for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, a US employer must offer them a job and submit a petition for a H-1B visa to the US immigration department. This is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position.\n",
    "\n",
    "This dataset contains H-1B petition data. The columns in the dataset include case status, employer name, worksite coordinates, job title, prevailing wage, occupation code, and year filed.\n",
    "\n",
    "For more information on individual columns, refer to the column metadata. A detailed description of the underlying raw dataset is available in an [official data dictionary](https://www.foreignlaborcert.doleta.gov/docs/Performance_Data/Disclosure/FY15-FY16/H-1B_FY16_Record_Layout.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my tutorial session, I'll use the data file `h1b_sample.csv` but would strongly recommed that you use the data file `h1b_learners.csv` to do the hands-on practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/h1b_sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read data into the SQLContext object, Spark:\n",
    "\n",
    "- Instantiates a Spark DataFrame object\n",
    "- Infers the schema from the data and associates it with the DataFrame\n",
    "- Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "- Returns the DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the **schema for the DataFrame** we created out of our dataset. For this, we can call `printSchema()` method on our dataframe. This will provide us the datatype of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CASE_STATUS: string (nullable = true)\n",
      " |-- EMPLOYER_NAME: string (nullable = true)\n",
      " |-- SOC_NAME: string (nullable = true)\n",
      " |-- JOB_TITLE: string (nullable = true)\n",
      " |-- FULL_TIME_POSITION: string (nullable = true)\n",
      " |-- PREVAILING_WAGE: string (nullable = true)\n",
      " |-- YEAR: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`show()`** method on a DataFrame can give us a quick look on rows of the DataFame. Use `show()` to display 5 Rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|          SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|         OMD USA LLC|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|       108493.0|2016|\n",
      "|          CERTIFIED|THE GLOBAL ALLIAN...|MARKETING MANAGERS|SENIOR VP, MARKET...|                 Y|       179774.0|2016|\n",
      "|          CERTIFIED|THE GATORADE COMPANY|MARKETING MANAGERS|SENIOR MANAGER, S...|                 Y|       123386.0|2016|\n",
      "|          CERTIFIED|BOOMERANG COMMERC...|MARKETING MANAGERS|BUSINESS DEVELOPM...|                 Y|       163877.0|2016|\n",
      "|          CERTIFIED|AMAZON WEB SERVIC...|MARKETING MANAGERS|SENIOR PRODUCT MA...|                 Y|       115565.0|2016|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "Let's have statistical view of our dataframe.\n",
    "\n",
    "We can use `describe(*cols)` method on a dataframe to compute statistics for numeric and string columns.\n",
    "\n",
    "This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "|summary|CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|   PREVAILING_WAGE|  YEAR|\n",
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "|  count|       5000|                5000|                5000|                5000|              5000|              5000|  5000|\n",
      "|   mean|       null|                null|                null|                null|              null|179312.78471200023|2016.0|\n",
      "| stddev|       null|                null|                null|                null|              null| 4653266.403375367|   0.0|\n",
      "|    min|  CERTIFIED|1-800-FLOWERS.COM...|ADMINISTRATIVE SE...|(UI) USER INTERFA...|                 N|               0.0|  2016|\n",
      "|    max|  WITHDRAWN|          ZYNGA INC.|       SALES MANGERS|WORLDWIDE PARTNER...|                 Y|           99900.0|  2016|\n",
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we used the `head()` method to return the first n rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of row objects. Spark needs to return row objects for certain methods, such as head(), collect() and take().\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'OMD USA LLC', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'ASSOCIATE DIRECTOR, DIGITAL ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'108493.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'THE GLOBAL ALLIANCE FOR TB DRUG DEVELOPMENT INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR VP, MARKET ACCESS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'179774.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'THE GATORADE COMPANY', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR MANAGER, SOCIAL MEDIA MARKETING & ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'123386.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'BOOMERANG COMMERCE, INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'BUSINESS DEVELOPMENT MANAGER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'163877.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'AMAZON WEB SERVICES, INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR PRODUCT MANAGER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'115565.0', YEAR=u'2016')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, print the first row out the five fetched rows. Then print the `EMPLOYER_NAME` for the first row entry.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'OMD USA LLC', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'ASSOCIATE DIRECTOR, DIGITAL ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'108493.0', YEAR=u'2016')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'OMD USA LLC'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)[0].EMPLOYER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSOCIATE DIRECTOR, DIGITAL ANALYTICS\n",
      "SENIOR VP, MARKET ACCESS\n",
      "SENIOR MANAGER, SOCIAL MEDIA MARKETING & ANALYTICS\n",
      "BUSINESS DEVELOPMENT MANAGER\n",
      "SENIOR PRODUCT MANAGER\n"
     ]
    }
   ],
   "source": [
    "first_five = df.head(5)\n",
    "for each_element in first_five:\n",
    "    print each_element.JOB_TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns\n",
    "In pandas, we pass a string into a single pair of brackets ([]) to select an individual column, and pass in a list to select multiple columns. For example:\n",
    "\n",
    "#### Pandas DataFrame\n",
    "df['age']\n",
    "\n",
    "df[['age', 'males']]\n",
    "\n",
    "Spark also allows us to use bracket notation. Pass in a list of string objects with column name to select any column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the age value for first five employees in the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[YEAR: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes being lazily evaluated like RDDs will only display the results of an operation when we call any action upon it. We can call the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|YEAR|\n",
      "+----+\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('YEAR').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Employer Name with their case status.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|       EMPLOYER_NAME|        CASE_STATUS|\n",
      "+--------------------+-------------------+\n",
      "|         OMD USA LLC|CERTIFIED-WITHDRAWN|\n",
      "|THE GLOBAL ALLIAN...|          CERTIFIED|\n",
      "|THE GATORADE COMPANY|          CERTIFIED|\n",
      "|BOOMERANG COMMERC...|          CERTIFIED|\n",
      "|AMAZON WEB SERVIC...|          CERTIFIED|\n",
      "|CORNING INCORPORATED|          CERTIFIED|\n",
      "|          SNAP, INC.|          CERTIFIED|\n",
      "|      ASTRAZENECA LP|          CERTIFIED|\n",
      "|STARBUCKS COFFEE ...|          CERTIFIED|\n",
      "|NXSTAGE MEDICAL, ...|          CERTIFIED|\n",
      "|         MAYO CLINIC|          CERTIFIED|\n",
      "|POSSIBLE WORLDWID...|          CERTIFIED|\n",
      "|WAL-MART ASSOCIAT...|          CERTIFIED|\n",
      "|CARNEGIE MELLON U...|          CERTIFIED|\n",
      "| FARIA SYSTEMS, INC.|          WITHDRAWN|\n",
      "|KOYO BEARINGS NOR...|          CERTIFIED|\n",
      "|UNIVERSITY OF HAWAII|          CERTIFIED|\n",
      "| FARIA SYSTEMS, INC.|          CERTIFIED|\n",
      "|         ACCUEN INC.|          CERTIFIED|\n",
      "|         MAYO CLINIC|          CERTIFIED|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------+\n",
      "|       EMPLOYER_NAME|        CASE_STATUS|\n",
      "+--------------------+-------------------+\n",
      "|         OMD USA LLC|CERTIFIED-WITHDRAWN|\n",
      "|THE GLOBAL ALLIAN...|          CERTIFIED|\n",
      "|THE GATORADE COMPANY|          CERTIFIED|\n",
      "|BOOMERANG COMMERC...|          CERTIFIED|\n",
      "|AMAZON WEB SERVIC...|          CERTIFIED|\n",
      "|CORNING INCORPORATED|          CERTIFIED|\n",
      "|          SNAP, INC.|          CERTIFIED|\n",
      "|      ASTRAZENECA LP|          CERTIFIED|\n",
      "|STARBUCKS COFFEE ...|          CERTIFIED|\n",
      "|NXSTAGE MEDICAL, ...|          CERTIFIED|\n",
      "|         MAYO CLINIC|          CERTIFIED|\n",
      "|POSSIBLE WORLDWID...|          CERTIFIED|\n",
      "|WAL-MART ASSOCIAT...|          CERTIFIED|\n",
      "|CARNEGIE MELLON U...|          CERTIFIED|\n",
      "| FARIA SYSTEMS, INC.|          WITHDRAWN|\n",
      "|KOYO BEARINGS NOR...|          CERTIFIED|\n",
      "|UNIVERSITY OF HAWAII|          CERTIFIED|\n",
      "| FARIA SYSTEMS, INC.|          CERTIFIED|\n",
      "|         ACCUEN INC.|          CERTIFIED|\n",
      "|         MAYO CLINIC|          CERTIFIED|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hint: Use select() to display required columns\n",
    "df.select('EMPLOYER_NAME', 'CASE_STATUS').show()\n",
    "df[['EMPLOYER_NAME', 'CASE_STATUS']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the total number of rows in our dataframe. We can use count() to give us total number of rows in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Rows containing NULL values**\n",
    "\n",
    "We can use `drop(how='any', thresh=None, subset=None)` method on our dataframe to drop rows with null values and return a new dataframe.\n",
    "\n",
    "**Parameters:**\t\n",
    "\n",
    "**how** – ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "**thresh** – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "\n",
    "**subset** – optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|          SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|         OMD USA LLC|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|       108493.0|2016|\n",
      "|          CERTIFIED|THE GLOBAL ALLIAN...|MARKETING MANAGERS|SENIOR VP, MARKET...|                 Y|       179774.0|2016|\n",
      "|          CERTIFIED|THE GATORADE COMPANY|MARKETING MANAGERS|SENIOR MANAGER, S...|                 Y|       123386.0|2016|\n",
      "|          CERTIFIED|BOOMERANG COMMERC...|MARKETING MANAGERS|BUSINESS DEVELOPM...|                 Y|       163877.0|2016|\n",
      "|          CERTIFIED|AMAZON WEB SERVIC...|MARKETING MANAGERS|SENIOR PRODUCT MA...|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|CORNING INCORPORATED|MARKETING MANAGERS|COMPETITIVE INTEL...|                 Y|       123698.0|2016|\n",
      "|          CERTIFIED|          SNAP, INC.|MARKETING MANAGERS|DIRECTOR, BUSINES...|                 Y|       124197.0|2016|\n",
      "|          CERTIFIED|      ASTRAZENECA LP|MARKETING MANAGERS|ONCOLOGY INSIGHT ...|                 Y|       120786.0|2016|\n",
      "|          CERTIFIED|STARBUCKS COFFEE ...|MARKETING MANAGERS|    BRAND MANAGER II|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|NXSTAGE MEDICAL, ...|MARKETING MANAGERS|ASSOCIATE PRODUCT...|                 Y|        91312.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|       101587.0|2016|\n",
      "|          CERTIFIED|POSSIBLE WORLDWID...|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|        77106.0|2016|\n",
      "|          CERTIFIED|WAL-MART ASSOCIAT...|MARKETING MANAGERS|SENIOR DIRECTOR, ...|                 Y|       179733.0|2016|\n",
      "|          CERTIFIED|CARNEGIE MELLON U...|MARKETING MANAGERS|DIRECTOR OF MARKE...|                 Y|        71718.0|2016|\n",
      "|          WITHDRAWN| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|KOYO BEARINGS NOR...|MARKETING MANAGERS|      LAUNCH MANAGER|                 Y|       102211.0|2016|\n",
      "|          CERTIFIED|UNIVERSITY OF HAWAII|MARKETING MANAGERS|PUBLIC INFO, PUBL...|                 N|        50352.0|2016|\n",
      "|          CERTIFIED| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|         ACCUEN INC.|MARKETING MANAGERS|  DIRECTOR, INSIGHTS|                 N|        69618.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|        90002.0|2016|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.na.drop()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Null values\n",
    "**What if don't want to drop entire row but just replace the null values?**\n",
    "\n",
    "`fillna(value, subset=None)` enables us to replace null values in our dataframe. We can optionally specify the set of columns into which we want to replace nul values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in all the columns\n",
    "df = df.fillna(0)\n",
    "# df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace null values only in the Columns `CASE_STATUS` and `EMPLOYER_NAME`. \n",
    "\n",
    "Hint: Use `fillna(value, subset=None)` and specify required column names in the subset parameter. For example - `df.fillna(0, subset=['a', 'b'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|          SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|         OMD USA LLC|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|       108493.0|2016|\n",
      "|          CERTIFIED|THE GLOBAL ALLIAN...|MARKETING MANAGERS|SENIOR VP, MARKET...|                 Y|       179774.0|2016|\n",
      "|          CERTIFIED|THE GATORADE COMPANY|MARKETING MANAGERS|SENIOR MANAGER, S...|                 Y|       123386.0|2016|\n",
      "|          CERTIFIED|BOOMERANG COMMERC...|MARKETING MANAGERS|BUSINESS DEVELOPM...|                 Y|       163877.0|2016|\n",
      "|          CERTIFIED|AMAZON WEB SERVIC...|MARKETING MANAGERS|SENIOR PRODUCT MA...|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|CORNING INCORPORATED|MARKETING MANAGERS|COMPETITIVE INTEL...|                 Y|       123698.0|2016|\n",
      "|          CERTIFIED|          SNAP, INC.|MARKETING MANAGERS|DIRECTOR, BUSINES...|                 Y|       124197.0|2016|\n",
      "|          CERTIFIED|      ASTRAZENECA LP|MARKETING MANAGERS|ONCOLOGY INSIGHT ...|                 Y|       120786.0|2016|\n",
      "|          CERTIFIED|STARBUCKS COFFEE ...|MARKETING MANAGERS|    BRAND MANAGER II|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|NXSTAGE MEDICAL, ...|MARKETING MANAGERS|ASSOCIATE PRODUCT...|                 Y|        91312.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|       101587.0|2016|\n",
      "|          CERTIFIED|POSSIBLE WORLDWID...|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|        77106.0|2016|\n",
      "|          CERTIFIED|WAL-MART ASSOCIAT...|MARKETING MANAGERS|SENIOR DIRECTOR, ...|                 Y|       179733.0|2016|\n",
      "|          CERTIFIED|CARNEGIE MELLON U...|MARKETING MANAGERS|DIRECTOR OF MARKE...|                 Y|        71718.0|2016|\n",
      "|          WITHDRAWN| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|KOYO BEARINGS NOR...|MARKETING MANAGERS|      LAUNCH MANAGER|                 Y|       102211.0|2016|\n",
      "|          CERTIFIED|UNIVERSITY OF HAWAII|MARKETING MANAGERS|PUBLIC INFO, PUBL...|                 N|        50352.0|2016|\n",
      "|          CERTIFIED| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|         ACCUEN INC.|MARKETING MANAGERS|  DIRECTOR, INSIGHTS|                 N|        69618.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|        90002.0|2016|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filterd = df.fillna(0, subset=['CASE_STATUS', 'EMPLOYER_NAME']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the possible categories in the case status?\n",
    "\n",
    "`distinct()`: Returns a new DataFrame containing the distinct rows in this DataFrame.\n",
    "\n",
    "So next, select the `CASE_STATUS` column and apply `distint()` method on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        CASE_STATUS|\n",
      "+-------------------+\n",
      "|          CERTIFIED|\n",
      "|CERTIFIED-WITHDRAWN|\n",
      "|          WITHDRAWN|\n",
      "|             DENIED|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('CASE_STATUS').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine Distinct `CASE_STATUS` count for each `EMPLOYER_NAME`\n",
    "\n",
    "For example, determine how many visa applications are certified under the employer name `SAMSUNG ELECTRONICS`\n",
    "\n",
    "We can use `crosstab()` method to get this done. **crosstab(col1, col2)** computes a pair-wise frequency table of the given columns.\n",
    "\n",
    "**Parameters:**\t\n",
    "\n",
    "**col1** – The name of the first column. Distinct items will make the first item of each row.\n",
    "\n",
    "**col2** – The name of the second column. Distinct items will make the column names of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.crosstab('EMPLOYER_NAME', 'CASE_STATUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the top Employers getting more visa applications into a Certified Status**\n",
    "\n",
    "Find out the top 10 companies having highest number of certified visa applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|     TEMPLE UNIVERSITY...|        2|                  0|     0|        0|\n",
      "|     SAMSUNG ELECTRONI...|        4|                  1|     0|        0|\n",
      "|     MERCURY INSURANCE...|        0|                  1|     0|        0|\n",
      "|              ACCUEN INC.|        1|                  0|     0|        0|\n",
      "|        AKDY IMPORTS, LLC|        1|                  0|     0|        0|\n",
      "|           SAFEGRAPH INC.|        2|                  0|     0|        0|\n",
      "|     PUMA NORTH AMERIC...|        1|                  0|     0|        0|\n",
      "|             CYIENT, INC.|        1|                  0|     0|        0|\n",
      "|     THE SHERWIN-WILLI...|        3|                  0|     1|        0|\n",
      "|     ITG SOFTWARE SOLU...|        0|                  1|     0|        0|\n",
      "|             ANTHEM, INC.|        3|                  0|     0|        0|\n",
      "|     GAVS TECHNOLOGIES...|        1|                  0|     0|        0|\n",
      "|     GLOBAL TRAVEL SOL...|        1|                  0|     0|        0|\n",
      "|     INTERACTIVE BROAD...|        4|                  0|     0|        0|\n",
      "|            POPSUGAR INC.|        1|                  0|     0|        0|\n",
      "|       BRIGHT MARKET, LLC|        1|                  0|     0|        0|\n",
      "|     NOBLE DRILLING SE...|        1|                  0|     0|        0|\n",
      "|               PROJECT:TF|        1|                  0|     0|        0|\n",
      "|           PEOPLEASE, LLC|        1|                  0|     0|        0|\n",
      "|              AASONN, LLC|        1|                  0|     0|        0|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|     CAPGEMINI FINANCI...|      112|                  1|     0|        1|\n",
      "|     CAPGEMINI FINANCI...|       80|                  1|     1|        0|\n",
      "|     SYNTEL CONSULTING...|       75|                  0|     0|        3|\n",
      "|     DELOITTE CONSULTI...|       66|                  0|     0|        2|\n",
      "|     TECH MAHINDRA (AM...|       57|                  0|     0|        0|\n",
      "|     AMAZON CORPORATE LLC|       46|                  1|     0|        9|\n",
      "|      VIRTUSA CORPORATION|       40|                  0|     0|        3|\n",
      "|     CAPGEMINI AMERICA...|       39|                  0|     0|        0|\n",
      "|     MANHATTAN ASSOCIA...|       38|                  0|     3|        0|\n",
      "|           FACEBOOK, INC.|       36|                  5|     0|        1|\n",
      "|            ACCENTURE LLP|       35|                  0|     0|        0|\n",
      "|            BLOOMBERG, LP|       29|                  0|     0|        0|\n",
      "|          EMC CORPORATION|       24|                  2|     0|        1|\n",
      "|     LINKEDIN CORPORATION|       22|                  5|     0|        1|\n",
      "|               APPLE INC.|       22|                  3|     2|        1|\n",
      "|     HITACHI CONSULTIN...|       21|                  7|     0|        0|\n",
      "|               SYNTEL INC|       20|                  1|     0|        1|\n",
      "|              YAHOO! INC.|       20|                  3|     1|        1|\n",
      "|              GOOGLE INC.|       19|                  8|     0|        0|\n",
      "|      DELL MARKETING L.P.|       17|                  4|     1|        0|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.orderBy(df2['CERTIFIED'].desc())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which `JOB_TITLE` got the highest number of certified visa applications?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------------+------+---------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "| SUPPORT ENGINEER ...|        2|                  0|     0|        0|\n",
      "| DIVISION SALES MA...|        0|                  0|     0|        1|\n",
      "| INTERNATIONAL OPE...|        1|                  0|     2|        0|\n",
      "| IT ENTERPRISE/SAP...|        1|                  0|     0|        0|\n",
      "| INFRASTRUCTURE EN...|        1|                  0|     0|        0|\n",
      "| ASSOCIATE DIRECTO...|        1|                  0|     0|        0|\n",
      "| CONSULTING MANAGE...|        1|                  0|     0|        0|\n",
      "| VICE PRESIDENT OF...|        1|                  0|     0|        0|\n",
      "| MANAGER, INFRASTR...|        1|                  0|     0|        0|\n",
      "| DIRECTOR OF BUSIN...|       14|                  1|     0|        1|\n",
      "| DIRECTOR OF OPERA...|        1|                  0|     0|        0|\n",
      "| DIRECTOR, STATIST...|        1|                  0|     0|        0|\n",
      "| INTERNATIONAL BUS...|        1|                  0|     0|        1|\n",
      "| SENIOR COMPUTER S...|        1|                  0|     0|        0|\n",
      "| SENIOR TECHNICAL ...|        0|                  0|     0|        1|\n",
      "| VICE PRESIDENT - ...|        0|                  1|     0|        0|\n",
      "| VICE PRESIDENT OF...|        1|                  2|     0|        0|\n",
      "|    RESEARCH DIRECTOR|        0|                  1|     0|        0|\n",
      "| ASSOC. VP, IT APP...|        0|                  1|     0|        0|\n",
      "| SALES & MARKETING...|        1|                  0|     0|        0|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.crosstab('JOB_TITLE', 'CASE_STATUS')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------------+------+---------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "|      PROJECT MANAGER|      215|                 21|     2|        8|\n",
      "|        SALES MANAGER|      146|                  5|     7|        9|\n",
      "|              MANAGER|      132|                  6|     1|        2|\n",
      "|      LEAD CONSULTANT|      121|                  0|     1|        1|\n",
      "|    MARKETING MANAGER|       67|                  1|     9|        2|\n",
      "|      PRODUCT MANAGER|       54|                  4|     1|        6|\n",
      "| BUSINESS DEVELOPM...|       52|                  3|     4|        1|\n",
      "| SOFTWARE DEVELOPM...|       51|                  1|     1|        2|\n",
      "|  ENGINEERING MANAGER|       33|                  7|     0|        1|\n",
      "|       SENIOR MANAGER|       31|                  1|     0|        1|\n",
      "| SENIOR PRODUCT MA...|       28|                  0|     0|        4|\n",
      "| CHIEF TECHNOLOGY ...|       27|                  1|     0|        1|\n",
      "|     DELIVERY MANAGER|       27|                  0|     0|        0|\n",
      "|      PROGRAM MANAGER|       26|                  3|     0|        1|\n",
      "|          TEAM LEADER|       25|                  0|     0|        0|\n",
      "|           IT MANAGER|       20|                  0|     0|        1|\n",
      "| MANAGER, SOFTWARE...|       19|                  5|     0|        1|\n",
      "|      ACCOUNT MANAGER|       19|                  0|     1|        1|\n",
      "|   IT PROJECT MANAGER|       19|                  6|     0|        0|\n",
      "|      SALES MANAGER 4|       18|                  0|     0|        0|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.orderBy(df3['CERTIFIED'].desc())\n",
    "df3.orderBy(df3['CERTIFIED'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on columns\n",
    "\n",
    "**Classify Application status for each job title as either `Certified` or `NON-CERTIFIED`.**\n",
    "\n",
    "Hint: For each row we can sum up the values of columns `CERTIFIED-WITHDRAWN` + `WITHDRAWN` + `DENIED` into one single column as `NON-CERTIFIED`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+--------------------------------------------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|((CERTIFIED-WITHDRAWN + WITHDRAWN) + DENIED)|\n",
      "+---------------------+---------+--------------------------------------------+\n",
      "|      PROJECT MANAGER|      215|                                          31|\n",
      "|        SALES MANAGER|      146|                                          21|\n",
      "|              MANAGER|      132|                                           9|\n",
      "|      LEAD CONSULTANT|      121|                                           2|\n",
      "|    MARKETING MANAGER|       67|                                          12|\n",
      "|      PRODUCT MANAGER|       54|                                          11|\n",
      "| BUSINESS DEVELOPM...|       52|                                           8|\n",
      "| SOFTWARE DEVELOPM...|       51|                                           4|\n",
      "|  ENGINEERING MANAGER|       33|                                           8|\n",
      "|       SENIOR MANAGER|       31|                                           2|\n",
      "| SENIOR PRODUCT MA...|       28|                                           4|\n",
      "| CHIEF TECHNOLOGY ...|       27|                                           2|\n",
      "|     DELIVERY MANAGER|       27|                                           0|\n",
      "|      PROGRAM MANAGER|       26|                                           4|\n",
      "|          TEAM LEADER|       25|                                           0|\n",
      "|           IT MANAGER|       20|                                           1|\n",
      "| MANAGER, SOFTWARE...|       19|                                           6|\n",
      "|   IT PROJECT MANAGER|       19|                                           6|\n",
      "|      ACCOUNT MANAGER|       19|                                           2|\n",
      "|      SALES MANAGER 4|       18|                                           0|\n",
      "+---------------------+---------+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.select(df4['JOB_TITLE_CASE_STATUS'], df4['CERTIFIED'], df4['CERTIFIED-WITHDRAWN']+df4['WITHDRAWN']+df4['DENIED'])\n",
    "\n",
    "df4.select(df4['JOB_TITLE_CASE_STATUS'], df4['CERTIFIED'], df4['CERTIFIED-WITHDRAWN']+df4['WITHDRAWN']+df4['DENIED']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|NON-CERTIFIED|\n",
      "+---------------------+---------+-------------+\n",
      "|      PROJECT MANAGER|      215|           31|\n",
      "|        SALES MANAGER|      146|           21|\n",
      "|              MANAGER|      132|            9|\n",
      "|      LEAD CONSULTANT|      121|            2|\n",
      "|    MARKETING MANAGER|       67|           12|\n",
      "|      PRODUCT MANAGER|       54|           11|\n",
      "| BUSINESS DEVELOPM...|       52|            8|\n",
      "| SOFTWARE DEVELOPM...|       51|            4|\n",
      "|  ENGINEERING MANAGER|       33|            8|\n",
      "|       SENIOR MANAGER|       31|            2|\n",
      "| SENIOR PRODUCT MA...|       28|            4|\n",
      "|     DELIVERY MANAGER|       27|            0|\n",
      "| CHIEF TECHNOLOGY ...|       27|            2|\n",
      "|      PROGRAM MANAGER|       26|            4|\n",
      "|          TEAM LEADER|       25|            0|\n",
      "|           IT MANAGER|       20|            1|\n",
      "| MANAGER, SOFTWARE...|       19|            6|\n",
      "|      ACCOUNT MANAGER|       19|            2|\n",
      "|   IT PROJECT MANAGER|       19|            6|\n",
      "|      SALES MANAGER 4|       18|            0|\n",
      "+---------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = df5.select('JOB_TITLE_CASE_STATUS', 'CERTIFIED', col(\"((CERTIFIED-WITHDRAWN + WITHDRAWN) + DENIED)\").alias(\"NON-CERTIFIED\"))\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the total number of `CERTIFIED` and `NON-CERTIFIED` applications in your dataframe.**\n",
    "\n",
    "Hint: Use aggregation function like sum() to compute total number in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4273.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_certified = float(df6.groupBy().sum('CERTIFIED').collect()[0][0])\n",
    "total_certified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, calculate total number of NON-CERTIFIED applications.\n",
    "\n",
    "# total_noncertified = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the problem: I have a Python function that iterates over my data, but going through each row in the dataframe takes several days. If I have a computing cluster with many nodes, how can I distribute this Python function in PySpark to speed up this process — maybe cut the total time down to less than a few hours — with the least amount of work?\n",
    "\n",
    "In other words, how do I turn a Python function into a Spark user defined function, or UDF?\n",
    "\n",
    "<img src = \"images/dataframe.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we make use of User Defined Functions on our Dataframes?\n",
    "\n",
    "Recall, what was the use of `map()` and `flatMap()` methods when we were operating on our RDDs. Basically these help us to apply the user defined functions on each partition of our RDD.\n",
    "\n",
    "Similarly, spark allow us to operate on dataframe using our custom functions.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define your custom function\n",
    "2. Register UDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share(s):\n",
    "  return (s / total_certified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a UDF\n",
    "\n",
    "- PySpark UDFs work in a similar way as the pandas .map() and .apply() methods for pandas series and dataframes. If I have a function that can use values from a row in the dataframe as input, then I can map it to the entire dataframe. The only difference is that with PySpark UDFs we have to specify the output data type.\n",
    "\n",
    "- As long as the python function’s output has a corresponding data type in Spark, it can be turned into a UDF. When registering UDFs, we have to specify the data type using the types from pyspark.sql.types. All the types supported by PySpark [can be found here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types).\n",
    "\n",
    "- udf(): Returns a **UDFRegistration** for UDF registration.\n",
    "\n",
    "- register(name, f, returnType=StringType): Registers a python function (including lambda function) as a **UDF** so it can be used in SQL statements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4273.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "sqlContext.udf.register(\"Employershare\", share)\n",
    "\n",
    "print total_certified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|\n",
      "+-------------------------+---------+\n",
      "|     CAPGEMINI FINANCI...|    112.0|\n",
      "|     CAPGEMINI FINANCI...|     80.0|\n",
      "|     SYNTEL CONSULTING...|     75.0|\n",
      "|     DELOITTE CONSULTI...|     66.0|\n",
      "|     TECH MAHINDRA (AM...|     57.0|\n",
      "|     AMAZON CORPORATE LLC|     46.0|\n",
      "|      VIRTUSA CORPORATION|     40.0|\n",
      "|     CAPGEMINI AMERICA...|     39.0|\n",
      "|     MANHATTAN ASSOCIA...|     38.0|\n",
      "|           FACEBOOK, INC.|     36.0|\n",
      "|            ACCENTURE LLP|     35.0|\n",
      "|            BLOOMBERG, LP|     29.0|\n",
      "|          EMC CORPORATION|     24.0|\n",
      "|               APPLE INC.|     22.0|\n",
      "|     LINKEDIN CORPORATION|     22.0|\n",
      "|     HITACHI CONSULTIN...|     21.0|\n",
      "|               SYNTEL INC|     20.0|\n",
      "|              YAHOO! INC.|     20.0|\n",
      "|              GOOGLE INC.|     19.0|\n",
      "|      DELL MARKETING L.P.|     17.0|\n",
      "+-------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = df2.select(df2.EMPLOYER_NAME_CASE_STATUS, df2.CERTIFIED.cast(\"float\"))\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|      %share|\n",
      "+-------------------------+------------+\n",
      "|     CAPGEMINI FINANCI...| 0.026211092|\n",
      "|     CAPGEMINI FINANCI...|  0.01872221|\n",
      "|     SYNTEL CONSULTING...|  0.01755207|\n",
      "|     DELOITTE CONSULTI...| 0.015445823|\n",
      "|     TECH MAHINDRA (AM...| 0.013339574|\n",
      "|     AMAZON CORPORATE LLC|  0.01076527|\n",
      "|      VIRTUSA CORPORATION| 0.009361105|\n",
      "|     CAPGEMINI AMERICA...| 0.009127077|\n",
      "|     MANHATTAN ASSOCIA...| 0.008893049|\n",
      "|           FACEBOOK, INC.| 0.008424995|\n",
      "|            ACCENTURE LLP| 0.008190966|\n",
      "|            BLOOMBERG, LP| 0.006786801|\n",
      "|          EMC CORPORATION|0.0056166626|\n",
      "|     LINKEDIN CORPORATION|0.0051486073|\n",
      "|               APPLE INC.|0.0051486073|\n",
      "|     HITACHI CONSULTIN...|  0.00491458|\n",
      "|              YAHOO! INC.|0.0046805525|\n",
      "|               SYNTEL INC|0.0046805525|\n",
      "|              GOOGLE INC.|0.0044465247|\n",
      "|      DELL MARKETING L.P.|0.0039784694|\n",
      "+-------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "share_udf = udf(share, FloatType())\n",
    "df8 = df7.select(\"EMPLOYER_NAME_CASE_STATUS\", share_udf(df7.CERTIFIED).alias(\"%share\"))\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here’s a small gotcha** — because Spark UDF doesn’t convert integers to floats, unlike Python function which works for both integers and floats, a Spark UDF will return a column of NULLs if the input data type doesn’t match the output data type, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|%share|\n",
      "+-------------------------+------+\n",
      "|     CAPGEMINI FINANCI...|  null|\n",
      "|     CAPGEMINI FINANCI...|  null|\n",
      "|     SYNTEL CONSULTING...|  null|\n",
      "|     DELOITTE CONSULTI...|  null|\n",
      "|     TECH MAHINDRA (AM...|  null|\n",
      "|     AMAZON CORPORATE LLC|  null|\n",
      "|      VIRTUSA CORPORATION|  null|\n",
      "|     CAPGEMINI AMERICA...|  null|\n",
      "|     MANHATTAN ASSOCIA...|  null|\n",
      "|           FACEBOOK, INC.|  null|\n",
      "|            ACCENTURE LLP|  null|\n",
      "|            BLOOMBERG, LP|  null|\n",
      "|          EMC CORPORATION|  null|\n",
      "|               APPLE INC.|  null|\n",
      "|     LINKEDIN CORPORATION|  null|\n",
      "|     HITACHI CONSULTIN...|  null|\n",
      "|               SYNTEL INC|  null|\n",
      "|              YAHOO! INC.|  null|\n",
      "|              GOOGLE INC.|  null|\n",
      "|      DELL MARKETING L.P.|  null|\n",
      "+-------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "share_integer_udf = udf(share, IntegerType())\n",
    "df9 = df7.select(\"EMPLOYER_NAME_CASE_STATUS\", share_integer_udf(df7.CERTIFIED).alias(\"%share\"))\n",
    "\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Spark Dataframe to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'EMPLOYER_NAME_CASE_STATUS', u'%share'], dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df9.toPandas()\n",
    "pandas_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.8 QUIZ \n",
    "\n",
    "**Q.1 Which of the following are common feature of RDD and DataFrame?**\n",
    "\n",
    "A. immutability\n",
    "\n",
    "B. in-memory\n",
    "\n",
    "C. resilient\n",
    "\n",
    "D. All of the above\n",
    "\n",
    "Answer - D\n",
    "\n",
    "**Q. 2. With the help of Spark SQL, we can query structured data as a distributed dataset (RDD).**\n",
    "\n",
    "A. TRUE\n",
    "\n",
    "B. FALSE\n",
    "\n",
    "Answer - A\n",
    "\n",
    "**Q.3 Can we change a particular row entry in a Dataframe?**\n",
    "\n",
    "A. TRUE\n",
    "\n",
    "B. FALSE\n",
    "\n",
    "Answer - B, Dataframes are immutable in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a temproary table view from a Dataframe, which can be further used to perform SQL queries on the data. In part 1, we saw operations using Dataframes. We will pick the same dataset and perform some basic SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a table out of `df2`**\n",
    "\n",
    "Hint: We can use `registerTempTable(name)` method on any dataframe to create a table out of it.\n",
    "\n",
    "**`registerTempTable(name)`**: Registers this RDD as a temporary table using the given name.\n",
    "\n",
    "- The lifetime of this temporary table is tied to the SQLContext that was used to create this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"visa_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform SQL queries through Spark?\n",
    "\n",
    "TO perform SQL Queries we can `SparkSession.sql(sqlQuery)` where `sqlQuery` can be any valid sql query.\n",
    "\n",
    "- **`SparkSession.sql(sqlQuery)`**: **Returns a DataFrame** representing the result of the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa = spark.sql(\"select * from visa_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check if the above query gave us the identical dataframe in the result!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_visa.collect()) == sorted(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|       EMPLOYER_NAME|CERTIFIED_COUNT|\n",
      "+--------------------+---------------+\n",
      "|CAPGEMINI FINANCI...|            112|\n",
      "|CAPGEMINI FINANCI...|             80|\n",
      "|SYNTEL CONSULTING...|             75|\n",
      "|DELOITTE CONSULTI...|             66|\n",
      "|TECH MAHINDRA (AM...|             57|\n",
      "|AMAZON CORPORATE LLC|             46|\n",
      "| VIRTUSA CORPORATION|             40|\n",
      "|CAPGEMINI AMERICA...|             39|\n",
      "|MANHATTAN ASSOCIA...|             38|\n",
      "|      FACEBOOK, INC.|             36|\n",
      "+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top10 companies getting visa approval (for all the years)\n",
    "spark.sql(\"SELECT EMPLOYER_NAME, count(EMPLOYER_NAME) as CERTIFIED_COUNT FROM visa_table where CASE_STATUS = 'CERTIFIED' GROUP BY EMPLOYER_NAME order by CERTIFIED_COUNT desc\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|        JOB_TITLE|Approved|\n",
      "+-----------------+--------+\n",
      "|  PROJECT MANAGER|     215|\n",
      "|    SALES MANAGER|     146|\n",
      "|          MANAGER|     132|\n",
      "|  LEAD CONSULTANT|     121|\n",
      "|MARKETING MANAGER|      67|\n",
      "+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT JOB_TITLE, count(*) as Approved FROM visa_table where CASE_STATUS = 'CERTIFIED' GROUP BY JOB_TITLE order by Approved desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in our dataset the Job Title as `OPERATIONS MANAGER` has got highest number of approvals.\n",
    "\n",
    "**Let's find out the `EMPLOYER_NAME` having the highest number of operations manager getting visa approved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|       EMPLOYER_NAME|Approved|\n",
      "+--------------------+--------+\n",
      "|BUILDING CLEANING...|       1|\n",
      "|DOW HEALTHCARE ST...|       1|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT EMPLOYER_NAME,count(*) as Approved FROM visa_table where CASE_STATUS = 'CERTIFIED' AND JOB_TITLE ='OPERATIONS MANAGER' GROUP BY EMPLOYER_NAME order by Approved desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, find out the approved applications having the highest paid salaries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----+------------------+-----------+\n",
      "|          businesses|   wage|            SOC_NAME|           JOB_TITLE|YEAR|FULL_TIME_POSITION|CASE_STATUS|\n",
      "+--------------------+-------+--------------------+--------------------+----+------------------+-----------+\n",
      "|AJINOMOTO WINDSOR...|99882.0|COMPUTER AND INFO...| ERP SYSTEMS MANAGER|2016|                 Y|  CERTIFIED|\n",
      "| OVERSTOCK.COM, INC.|99861.0|COMPUTER AND INFO...|SOFTWARE DEVELOPM...|2016|                 Y|  CERTIFIED|\n",
      "|TECH MAHINDRA (AM...|99861.0|COMPUTER AND INFO...|    DELIVERY MANAGER|2016|                 Y|  CERTIFIED|\n",
      "|SELECT PORTFOLIO ...|99861.0|COMPUTER AND INFO...| ENGINEERING MANAGER|2016|                 Y|  CERTIFIED|\n",
      "|           HZO, INC.|99861.0|COMPUTER AND INFO...|APPLICATIONS DIRE...|2016|                 Y|  CERTIFIED|\n",
      "| OVERSTOCK.COM, INC.|99861.0|COMPUTER AND INFO...|    DEVELOPMENT LEAD|2016|                 Y|  CERTIFIED|\n",
      "| OVERSTOCK.COM, INC.|99861.0|COMPUTER AND INFO...|    DEVELOPMENT LEAD|2016|                 Y|  CERTIFIED|\n",
      "| VIRTUSA CORPORATION|99861.0|COMPUTER AND INFO...|      SENIOR MANAGER|2016|                 Y|  CERTIFIED|\n",
      "|   EULER CAPITAL LLC|99840.0|PUBLIC RELATIONS ...|PUBLIC RELATIONS ...|2016|                 Y|  CERTIFIED|\n",
      "|  CAPGEMINI U.S. LLC|99800.0|COMPUTER AND INFO...|MANAGER/PROJECT M...|2016|                 Y|  CERTIFIED|\n",
      "+--------------------+-------+--------------------+--------------------+----+------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT EMPLOYER_NAME as businesses, PREVAILING_WAGE as wage, SOC_NAME, JOB_TITLE, YEAR, FULL_TIME_POSITION, CASE_STATUS  FROM visa_table where CASE_STATUS ='CERTIFIED' order by PREVAILING_WAGE desc\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame Exercise\n",
    "\n",
    "Given a stock market dataset come up with short analysis. Answer basic questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the following file as a input:\n",
    "\n",
    "`data/appl_stock.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 - Load the Apple Stock CSV File, make sure you infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/appl_stock.csv', inferSchema=True, header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 - What are the column names and Schema?\n",
    "\n",
    "Hint: use `columns` and `printSchema()` on your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 - Print out the first 5 rows.\n",
    "\n",
    "Hint: `take(n)` helps in selectinh first n rows in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039),\n",
       " Row(Date=datetime.datetime(2010, 1, 5, 0, 0), Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976000000002),\n",
       " Row(Date=datetime.datetime(2010, 1, 6, 0, 0), Open=214.379993, High=215.23, Low=210.750004, Close=210.969995, Volume=138040000, Adj Close=27.333178000000004),\n",
       " Row(Date=datetime.datetime(2010, 1, 7, 0, 0), Open=211.75, High=212.000006, Low=209.050005, Close=210.58, Volume=119282800, Adj Close=27.28265),\n",
       " Row(Date=datetime.datetime(2010, 1, 8, 0, 0), Open=210.299994, High=212.000006, Low=209.06000500000002, Close=211.98000499999998, Volume=111902700, Adj Close=27.464034)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "Use `describe()` to learn about the Dataframe.\n",
    "There are too many decimal places for `mean` and `stddev` in the `describe()` dataframe.\n",
    "Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that `.describe()` returns, we didn't cover how to do this exact formatting, but we covered something very similar. [Check this link for a hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast)\n",
    "\n",
    "\n",
    "Hint: Pyspark sql function `format_number` helps in casting a column of any Dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|summary|              Open|              High|               Low|            Close|             Volume|         Adj Close|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|  count|              1762|              1762|              1762|             1762|               1762|              1762|\n",
      "|   mean| 313.0763111589103| 315.9112880164581| 309.8282405079457|312.9270656379113|9.422577587968218E7| 75.00174115607275|\n",
      "| stddev|185.29946803981522|186.89817686485767|183.38391664371008|185.1471036170943|6.020518776592709E7| 28.57492972179906|\n",
      "|    min|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|\n",
      "|    max|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().printSchema()\n",
    "desc = df.describe()\n",
    "desc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+--------+--------------+\n",
      "|summary|    Open|    High|     Low|   Close|     Adj Close|\n",
      "+-------+--------+--------+--------+--------+--------------+\n",
      "|  count|1,762.00|1,762.00|1,762.00|1,762.00|      1,762.00|\n",
      "|   mean|  313.08|  315.91|  309.83|  312.93| 94,225,776.00|\n",
      "| stddev|  185.30|  186.90|  183.38|  185.15| 60,205,188.00|\n",
      "|    min|   90.00|   90.70|   89.47|   90.28| 11,475,900.00|\n",
      "|    max|  702.41|  705.07|  699.57|  702.10|470,249,504.00|\n",
      "+-------+--------+--------+--------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "'''Formats numeric column x to a format like '#,###,###.##', \n",
    "rounded to d decimal places, and returns the result as a string column.'''\n",
    "\n",
    "\n",
    "desc.select(desc['summary'], format_number(desc['Open'].cast('float'), 2).alias('Open'), \n",
    "            format_number(desc['High'].cast('float'), 2).alias('High'),\n",
    "            format_number(desc['Low'].cast('float'), 2).alias('Low'), \n",
    "            format_number(desc['Close'].cast('float'), 2).alias('Close'),\n",
    "            format_number(desc['Volume'].cast('float'), 2).alias('Adj Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "\n",
    "What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2012, 9, 21, 0, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df['High'].desc()).take(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "What is the mean of the Close column?\n",
    "\n",
    "Hint: Import and use avg/mean function of pyspak.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|312.9270656379113|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df.select(avg('Close')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|312.9270656379113|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another way to do it\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean('Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "What is the max and min of the Volume column?\n",
    "\n",
    "Hint: Import and use min/max function of pyspak.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|  470249500|   11475900|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(max(df['Volume']), min(df['Volume'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "What percentage of the time was the High greater than 150 dollars ?\n",
    "#### In other words, \n",
    "\n",
    "> (Number of Days High > 150) / (Total Days in the dataset)\n",
    "\n",
    "Hint: Remeber to take a distinct count of dates to calculate number of Days. Filter `High` column for `values>150` and then apply countDistinct(`date_column`) and similary calculate total number of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.223609534619754"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "float(df.filter(df['High'] > 150).select(countDistinct(df['Date'])).collect()[0][0]) / df.select(countDistinct(df['Date'])).collect()[0][0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.223609534619754"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another simple way to do it\n",
    "float(df.filter(df['High'] > 150).count()) / df.count() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "\n",
    "What is the max High per year?\n",
    "\n",
    "Hint - Import `year` from pyspark.sql.functions to generate year column for each date and then call groupBy('Year') and finally find max value out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|               Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|Year|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|2010|\n",
      "+-------------------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "year_df = df.withColumn('Year', year(df['Date']))\n",
    "year_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|         max(High)|\n",
      "+----+------------------+\n",
      "|2015|134.53999299999998|\n",
      "|2013|        575.139999|\n",
      "|2014|        651.259979|\n",
      "|2012|        705.070023|\n",
      "|2016|        118.690002|\n",
      "|2010|            326.66|\n",
      "|2011|426.69999299999995|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year_df.groupBy('Year').max().select(['Year', 'max(High)']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Power of MLlib:Apache Spark\n",
    "\n",
    "# 8.1 Why Machine Learning with Spark?\n",
    "\n",
    "- One of the major attractions of Spark is the ability to **scale computation massively**, and that is exactly what you need for machine learning algorithms. But the **limitation is that all machine learning algorithms cannot be effectively parallelized**. Each algorithm has its own challenges for parallelization, whether it is task parallelism or data parallelism.\n",
    "\n",
    "- Having said that, Spark is becoming the de-facto platform for building machine learning algorithms and applications. The developers working on the Spark MLlib are implementing more and more machine algorithms in a scalable and concise manner in the Spark framework.\n",
    "\n",
    "## Companies making Buisness out of SparkMLlib\n",
    "\n",
    "Many compelling business scenarios and technical solutions are being solved today with Spark MLlib, including [Huawei on Frequent Pattern Mining](https://databricks.com/blog/2015/06/09/huawei-embraces-open-source-apache-spark.html), [OpenTable’s Dining Recommendations](https://www.slideshare.net/SparkSummit/using-data-science-to-transform-opentable-into-delgado-das). Some additional examples:\n",
    "\n",
    "- [NBC Universal](https://databricks.com/session/use-of-spark-mllib-for-predicting-the-offlining-of-digital-media) stores hundreds of terabytes of media for international cable TV. To save on costs, it takes the media offline when it is unlikely to be used soon. The company uses Spark MLlib Support Vector Machines to predict which files will not be used.\n",
    "\n",
    "- [The Toyota Customer](https://databricks.com/session/data-driven-toyota-customer-360-insights-on-apache-spark-and-mllib) 360 Insights Platform and Social Media Intelligence Center is powered by Spark MLlib. Toyota uses MLlib to categorize and prioritize social media interactions in real-time.\n",
    "\n",
    "- [Radius Intelligence](https://pages.databricks.com/CaseStudy-Raidus.html) uses Spark MLlib to process billions of data points from customers and external data sources, including 25 million canonical businesses and hundreds of millions of business listings from various sources.\n",
    "\n",
    "- [ING](https://databricks.com/session/real-time-anomaly-detection-with-spark-ml-and-akka) uses Spark in its data analytics pipeline for anomaly detection. The company’s machine learning pipeline uses Spark decision tree ensembles and k-means clustering.\n",
    "\n",
    "### Spark is there in every industry!!\n",
    "\n",
    "<img src=\"images/spark_usecases.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some Predictive Analysis with Spark MLlib\n",
    "\n",
    "**What Predictive analysis demands?**\n",
    "- Predictive analytics answers a question, but to accurately answer that question, you must have reliable historical data.\n",
    "\n",
    "**Why is this important?**\n",
    "- Because computers learn from historical data to make their predictions. Think of it like this — thinking of how we learned things (i.e. how we learned colors, animal names), a similar learning process is done in machine learning. Someone taught us color names, identifying animal with names. Similarly, in machine learning, we have to provide accurate examples via historical data so that the **machine learning process learns from the historical data to train a classification model**.\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/ml.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the problem\n",
    "\n",
    "We will be solving a classification problem. Using Bank Marketing dataset, the classification goal is to predict whether the client will subscribe (Yes/No) to a term deposit depending on other client related details.\n",
    "\n",
    "## Introduction to the Dataset\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n",
    "Attribute Information:\n",
    "\n",
    "**Input variables: Bank client data**\n",
    "\n",
    "1 - **age** (numeric)\n",
    "\n",
    "2 - **job** : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "\n",
    "3 - **marital** : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "\n",
    "4 - **education** (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "\n",
    "5 - **default**: has credit in default? (categorical: 'no','yes','unknown')\n",
    "\n",
    "6 - **housing**: has housing loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "7 - **loan**: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "**related with the last contact of the current campaign:**\n",
    "\n",
    "8 - **contact**: contact communication type (categorical: 'cellular','telephone') \n",
    "\n",
    "9 - **month**: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "\n",
    "10 - **day_of_week**: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "\n",
    "11 - **duration**: last contact duration, in seconds (numeric). Important note: this attribute highly affects the \n",
    "output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "**other attributes:**\n",
    "\n",
    "12 - **campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "\n",
    "13 - **pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "\n",
    "14 - **previous**: number of contacts performed before this campaign and for this client (numeric)\n",
    "\n",
    "15 - **poutcome**: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "**Output variable: deposit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "df = spark.read.csv('data/bank.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|       job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "| 59|    admin.|married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
      "| 56|    admin.|married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
      "| 41|technician|married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|\n",
      "| 55|  services|married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|\n",
      "| 54|    admin.|married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to see in a nice formatted table?\n",
    "Try it out with pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         job  marital  education default  balance housing loan  contact  \\\n",
       "0   59      admin.  married  secondary      no     2343     yes   no  unknown   \n",
       "1   56      admin.  married  secondary      no       45      no   no  unknown   \n",
       "2   41  technician  married  secondary      no     1270     yes   no  unknown   \n",
       "3   55    services  married  secondary      no     2476     yes   no  unknown   \n",
       "4   54      admin.  married   tertiary      no      184      no   no  unknown   \n",
       "\n",
       "   day month  duration  campaign  pdays  previous poutcome deposit  \n",
       "0    5   may      1042         1     -1         0  unknown     yes  \n",
       "1    5   may      1467         1     -1         0  unknown     yes  \n",
       "2    5   may      1389         1     -1         0  unknown     yes  \n",
       "3    5   may       579         1     -1         0  unknown     yes  \n",
       "4    5   may       673         2     -1         0  unknown     yes  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our data is evenly distributed into these two categories i.e. `yes` and `no`\n",
    "\n",
    "**Count total clients subscribed with the deposit. In other words  calculate number of client with deposit value as `yes`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5289"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hint: Use filter on deposit column\n",
    "df.filter(df['deposit']=='yes').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly calculate total number of clients who did not subscribed to deposit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5873"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['deposit']=='no').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with the counts you can see our data is quite evenly distributed which means the classes are balanced :)\n",
    "\n",
    "Let's start building model with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+--------+---------+-------+------------------+-------+-----+--------+------------------+-----+------------------+------------------+------------------+------------------+--------+-------+\n",
      "|summary|               age|    job| marital|education|default|           balance|housing| loan| contact|               day|month|          duration|          campaign|             pdays|          previous|poutcome|deposit|\n",
      "+-------+------------------+-------+--------+---------+-------+------------------+-------+-----+--------+------------------+-----+------------------+------------------+------------------+------------------+--------+-------+\n",
      "|  count|             11162|  11162|   11162|    11162|  11162|             11162|  11162|11162|   11162|             11162|11162|             11162|             11162|             11162|             11162|   11162|  11162|\n",
      "|   mean|41.231947679627304|   null|    null|     null|   null|1528.5385235620856|   null| null|    null|15.658036194230425| null|371.99381831213043| 2.508421429851281| 51.33040673714388|0.8325568894463358|    null|   null|\n",
      "| stddev|11.913369192215518|   null|    null|     null|   null| 3225.413325946149|   null| null|    null| 8.420739541006462| null|347.12838571630687|2.7220771816614824|108.75828197197717| 2.292007218670508|    null|   null|\n",
      "|    min|                18| admin.|divorced|  primary|     no|             -6847|     no|   no|cellular|                 1|  apr|                 2|                 1|                -1|                 0| failure|     no|\n",
      "|    max|                95|unknown|  single|  unknown|    yes|             81204|    yes|  yes| unknown|                31|  sep|              3881|                63|               854|                58| unknown|    yes|\n",
      "+-------+------------------+-------+--------+---------+-------+------------------+-------+-----+--------+------------------+-----+------------------+------------------+------------------+------------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some observations done, Day and month columns are not really important in this classification problem. So, we will remove there two columns and create a new dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Workflow\n",
    "\n",
    "A typical standard machine learning workflow is as follows:\n",
    "\n",
    "1. Loading data (aka data ingestion)\n",
    "\n",
    "2. Extracting features (aka feature extraction)\n",
    "\n",
    "3. Training model (aka model training)\n",
    "\n",
    "4. Evaluate (or predictionize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Machine Learning\n",
    "\n",
    "The process includes Category Indexing, One-Hot Encoding and VectorAssembler — a feature transformer that merges multiple columns into a vector column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use OneHotEncoderEstimator, StringIndexer, VectorAssembler**\n",
    "\n",
    "**Need help?**\n",
    "\n",
    "**1. StringIndexer:** StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels], ordered by label frequencies, so the most frequent label gets index 0. If the input column is numeric, we cast it to string and index the string values. you must set the input column of the component to this string-indexed column name. In many cases, you can set the input column with setInputCol. \n",
    "\n",
    "**2. OneHotEncoder:** One-hot Encoding: One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default because it makes the vector entries sum up to one so an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].\n",
    "\n",
    "**3. VectorAssembler:** VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id|gender|genderIndex|\n",
      "+---+------+-----------+\n",
      "|  0|     M|        0.0|\n",
      "|  1|     F|        1.0|\n",
      "|  2|     F|        1.0|\n",
      "|  3|     M|        0.0|\n",
      "|  4|     M|        0.0|\n",
      "|  5|     M|        0.0|\n",
      "+---+------+-----------+\n",
      "\n",
      "+---+-------------+\n",
      "| id|    genderVec|\n",
      "+---+-------------+\n",
      "|  0|(2,[0],[1.0])|\n",
      "|  1|(2,[1],[1.0])|\n",
      "|  2|(2,[1],[1.0])|\n",
      "|  3|(2,[0],[1.0])|\n",
      "|  4|(2,[0],[1.0])|\n",
      "|  5|(2,[0],[1.0])|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#StringIndexer Example\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df1 = spark.createDataFrame([(0, \"M\"), (1, \"F\"), (2, \"F\"), (3, \"M\"), (4, \"M\"), (5, \"M\")],[\"id\", \"gender\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"genderIndex\")\n",
    "\n",
    "indexed = indexer.fit(df1).transform(df1)\n",
    "\n",
    "indexed.show()\n",
    "\n",
    "#OneHotEncoderEstimator\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df1 = spark.createDataFrame([(0, \"M\"), (1, \"F\"), (2, \"F\"), (3, \"M\"), (4, \"M\"), (5, \"M\")],[\"id\", \"gender\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"gender\", outputCol=\"genderIndex\")\n",
    "\n",
    "model = stringIndexer.fit(df1)\n",
    "\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"genderIndex\", outputCol=\"genderVec\")\n",
    "\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "encoded.select(\"id\", \"genderVec\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look in detail what's happening**\n",
    "\n",
    "- It indexes each categorical column using the StringIndexer, then converts the indexed categories into one-hot encoded variables. \n",
    "\n",
    "- The resulting output has the binary vectors appended to the end of each row. We use the StringIndexer again to encode our labels to label indices.\n",
    "\n",
    "- Next, we use the VectorAssembler to combine all the feature columns into a single vector column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want more help in understanding `OneHotEncoderEstimator`?\n",
    "\n",
    "**Excellent explanation given in below link:**\n",
    "\n",
    "-> **[OneHotEncoderEstimator](https://stackoverflow.com/questions/42295001/how-to-interpret-results-of-spark-onehotencoder)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ML Pipelines\n",
    "\n",
    "ML Pipeline API lets Spark users quickly and easily assemble and configure practical distributed Machine Learning pipelines by standardizing the APIs for different Machine Learning concepts. A ML pipeline (or a ML workflow) is a sequence of Transformers and Estimators to fit a PipelineModel to an input dataset.\n",
    "\n",
    "\n",
    "<img src = \"images/pipeline.png\">\n",
    "\n",
    "### Features of Pipeline API\n",
    "The features of the Pipeline API in Spark MLlib:\n",
    "- DataFrame as a dataset format\n",
    "- ML Pipelines API is similar to scikit-learn\n",
    "- Easy debugging (via inspecting columns added during execution)\n",
    "- Parameter tuning\n",
    "- Compositions (to build more complex pipelines out of existing ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>59</td>\n",
       "      <td>56</td>\n",
       "      <td>41</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>admin.</td>\n",
       "      <td>admin.</td>\n",
       "      <td>technician</td>\n",
       "      <td>services</td>\n",
       "      <td>admin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital</th>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "      <td>tertiary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>2343</td>\n",
       "      <td>45</td>\n",
       "      <td>1270</td>\n",
       "      <td>2476</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>1042</td>\n",
       "      <td>1467</td>\n",
       "      <td>1389</td>\n",
       "      <td>579</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdays</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poutcome</th>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deposit</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0  \\\n",
       "label                                                      1   \n",
       "features   (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "age                                                       59   \n",
       "job                                                   admin.   \n",
       "marital                                              married   \n",
       "education                                          secondary   \n",
       "default                                                   no   \n",
       "balance                                                 2343   \n",
       "housing                                                  yes   \n",
       "loan                                                      no   \n",
       "contact                                              unknown   \n",
       "duration                                                1042   \n",
       "campaign                                                   1   \n",
       "pdays                                                     -1   \n",
       "previous                                                   0   \n",
       "poutcome                                             unknown   \n",
       "deposit                                                  yes   \n",
       "\n",
       "                                                           1  \\\n",
       "label                                                      1   \n",
       "features   (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "age                                                       56   \n",
       "job                                                   admin.   \n",
       "marital                                              married   \n",
       "education                                          secondary   \n",
       "default                                                   no   \n",
       "balance                                                   45   \n",
       "housing                                                   no   \n",
       "loan                                                      no   \n",
       "contact                                              unknown   \n",
       "duration                                                1467   \n",
       "campaign                                                   1   \n",
       "pdays                                                     -1   \n",
       "previous                                                   0   \n",
       "poutcome                                             unknown   \n",
       "deposit                                                  yes   \n",
       "\n",
       "                                                           2  \\\n",
       "label                                                      1   \n",
       "features   (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "age                                                       41   \n",
       "job                                               technician   \n",
       "marital                                              married   \n",
       "education                                          secondary   \n",
       "default                                                   no   \n",
       "balance                                                 1270   \n",
       "housing                                                  yes   \n",
       "loan                                                      no   \n",
       "contact                                              unknown   \n",
       "duration                                                1389   \n",
       "campaign                                                   1   \n",
       "pdays                                                     -1   \n",
       "previous                                                   0   \n",
       "poutcome                                             unknown   \n",
       "deposit                                                  yes   \n",
       "\n",
       "                                                           3  \\\n",
       "label                                                      1   \n",
       "features   (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "age                                                       55   \n",
       "job                                                 services   \n",
       "marital                                              married   \n",
       "education                                          secondary   \n",
       "default                                                   no   \n",
       "balance                                                 2476   \n",
       "housing                                                  yes   \n",
       "loan                                                      no   \n",
       "contact                                              unknown   \n",
       "duration                                                 579   \n",
       "campaign                                                   1   \n",
       "pdays                                                     -1   \n",
       "previous                                                   0   \n",
       "poutcome                                             unknown   \n",
       "deposit                                                  yes   \n",
       "\n",
       "                                                           4  \n",
       "label                                                      1  \n",
       "features   (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "age                                                       54  \n",
       "job                                                   admin.  \n",
       "marital                                              married  \n",
       "education                                           tertiary  \n",
       "default                                                   no  \n",
       "balance                                                  184  \n",
       "housing                                                   no  \n",
       "loan                                                      no  \n",
       "contact                                              unknown  \n",
       "duration                                                 673  \n",
       "campaign                                                   2  \n",
       "pdays                                                     -1  \n",
       "previous                                                   0  \n",
       "poutcome                                             unknown  \n",
       "deposit                                                  yes  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have features column and label column.\n",
    "\n",
    "**Let's randomly split data into train and test sets, and set seed for reproducibility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7764\n",
      "Test Dataset Count: 3398\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model over the training set, we can also obtain the receiver-operating characteristic and areaUnderROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set areaUnderROC: 0.884909242115\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision and recall.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "|age|       job|label|       rawPrediction|prediction|         probability|\n",
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "| 37|management|  0.0|[1.19871810716723...|       0.0|[0.76829666339830...|\n",
      "| 40|management|  0.0|[2.20534940465796...|       0.0|[0.90072886169926...|\n",
      "| 53|management|  0.0|[1.02590348276690...|       0.0|[0.73612093009497...|\n",
      "| 32|management|  0.0|[1.25795481657702...|       0.0|[0.77867383994058...|\n",
      "| 54|management|  0.0|[1.33232096924268...|       0.0|[0.79122429116078...|\n",
      "| 40|management|  0.0|[1.57095096412779...|       0.0|[0.82791913346617...|\n",
      "| 56|management|  0.0|[3.06095963426752...|       0.0|[0.95525333386804...|\n",
      "| 50|management|  0.0|[-0.8102603273804...|       1.0|[0.30783502428597...|\n",
      "| 47|management|  0.0|[0.67024288891379...|       0.0|[0.66155754396054...|\n",
      "| 44|management|  0.0|[1.29756265761715...|       0.0|[0.78542449653716...|\n",
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test Area Under ROC', 0.8858324614449619)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+--------------+----------+--------------------+\n",
      "|age|       job|label| rawPrediction|prediction|         probability|\n",
      "+---+----------+-----+--------------+----------+--------------------+\n",
      "| 37|management|  0.0|[1130.0,387.0]|       0.0|[0.74489123269611...|\n",
      "| 40|management|  0.0| [1333.0,86.0]|       0.0|[0.93939393939393...|\n",
      "| 53|management|  0.0|[1130.0,387.0]|       0.0|[0.74489123269611...|\n",
      "| 32|management|  0.0|[1130.0,387.0]|       0.0|[0.74489123269611...|\n",
      "| 54|management|  0.0| [1333.0,86.0]|       0.0|[0.93939393939393...|\n",
      "| 40|management|  0.0|  [373.0,30.0]|       0.0|[0.92555831265508...|\n",
      "| 56|management|  0.0| [1333.0,86.0]|       0.0|[0.93939393939393...|\n",
      "| 50|management|  0.0|[788.0,1230.0]|       1.0|[0.39048562933597...|\n",
      "| 47|management|  0.0|[788.0,1230.0]|       1.0|[0.39048562933597...|\n",
      "| 44|management|  0.0|[1130.0,387.0]|       0.0|[0.74489123269611...|\n",
      "+---+----------+-----+--------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate our Decision Tree model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.780724005007\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple decision tree performed poorly because it is too weak given the range of different features. The prediction accuracy of decision trees can be improved by Ensemble methods, such as Random Forest and Gradient-Boosted Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "|age|       job|label|       rawPrediction|prediction|         probability|\n",
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "| 37|management|  0.0|[13.7684085682463...|       0.0|[0.68842042841231...|\n",
      "| 40|management|  0.0|[15.8223745371936...|       0.0|[0.79111872685968...|\n",
      "| 53|management|  0.0|[13.1702598030275...|       0.0|[0.65851299015137...|\n",
      "| 32|management|  0.0|[14.1919959887199...|       0.0|[0.70959979943599...|\n",
      "| 54|management|  0.0|[14.6398539567788...|       0.0|[0.73199269783894...|\n",
      "| 40|management|  0.0|[14.0699607906075...|       0.0|[0.70349803953037...|\n",
      "| 56|management|  0.0|[18.2300150701502...|       0.0|[0.91150075350751...|\n",
      "| 50|management|  0.0|[5.87833809369094...|       1.0|[0.29391690468454...|\n",
      "| 47|management|  0.0|[9.42210757136227...|       1.0|[0.47110537856811...|\n",
      "| 44|management|  0.0|[11.9323409071916...|       0.0|[0.59661704535958...|\n",
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate our Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.882371041465\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the problem\n",
    "\n",
    "**Problem Satatement**\n",
    "\n",
    "Using a dataset of collection of SMS, predict whether a piece of text/sms is a Spam or not\n",
    "\n",
    "**Exploring Dataset**\n",
    "\n",
    "1. Read the dataset into a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark dataframe basic example\") \\\n",
    "#     .getOrCreate()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = spark.read.csv('data/SMSSpamCollection.csv', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the column names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category', 'text']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's have a look to our Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|category|                text|\n",
      "+--------+--------------------+\n",
      "|     ham|Go until jurong p...|\n",
      "|     ham|Ok lar... Joking ...|\n",
      "|    spam|Free entry in 2 a...|\n",
      "|     ham|U dun say so earl...|\n",
      "|     ham|Nah I don't think...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 - Print out the schema to see datatype of each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 - Count number of messages in each catergory present into our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|     ham| 4827|\n",
      "|    spam|  747|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# by top 20 categories\n",
    "data.groupBy(\"category\") \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3 - Next, perform three tasks:**\n",
    "\n",
    "- Prepare your data i.e. text column by tokenizing the words\n",
    "- Remove stop words from the tokenized column. You can use nltk `english` collection of stopwords to intializae a set of stopwords. For example: \n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "add_stopwords = stopwords.words('english')\n",
    "StopWordsRemover(inputCol, outputCol).setStopWords(add_stopwords)\n",
    "```\n",
    "\n",
    "- Finally Bag of words i.e. CountVectorizer to get features\n",
    "\n",
    "Use official documentation guide Spark on MLlib to solve the exercise. Link:\n",
    "\n",
    "[Spark MLlib guide](http://spark.apache.org/docs/1.6.2/ml-guide.html)\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "1.  From `pyspark.ml.feature import RegexTokenizer` and give `text` column as input to it, correspondinly save output in a new column.\n",
    "\n",
    "2. From `pyspark.ml.feature import StopWordsRemover` and give the tokenized set of words column as an input to it.\n",
    "\n",
    "3. from pyspark.ml.feature import CountVectorizer and give the stopwords filtered set of column as input to it and output column will be the feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "\n",
    "# stop words\n",
    "add_stopwords = stopwords.words('english')\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 - Next, Apply StringIndexer to the `category` column to get label indexing**\n",
    "\n",
    "Hint: Use `from pyspark.ml.feature import StringIndexer` by giving Input column as our target column which is `category` and name the output column as you like,, say `label`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# String Indexer - A label indexer that maps a string column of labels to an ML column of label indices.\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 - Pipeline All the steps performed above using Spark Pipeline API**\n",
    "\n",
    "Hint: Use `from pyspark.ml import Pipeline` and give stages as set of operations we want to perform in ordered series. Stages list will look like, stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `show()` to have a look to your transformed dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|category|                text|               words|            filtered|         rawFeatures|            features|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|     ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(10000,[165,1150,...|(10000,[165,1150,...|  0.0|\n",
      "|     ham|Ok lar... Joking ...|[ok, lar, joking,...|[ok, lar, joking,...|(10000,[20,2484,5...|(10000,[20,2484,5...|  0.0|\n",
      "|    spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|(10000,[253,1073,...|(10000,[253,1073,...|  1.0|\n",
      "|     ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(10000,[1535,3722...|(10000,[1535,3722...|  0.0|\n",
      "|     ham|Nah I don't think...|[nah, i, don, t, ...|[nah, think, goes...|(10000,[1210,1564...|(10000,[1210,1564...|  0.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6 - Next, split the dataset into training and test data**\n",
    "\n",
    "**Also, count the number of rows into each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3964\n",
      "Test Dataset Count: 1610\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using Count Vector Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7 - Apply Logisitic Regression using Count Vector Features generated and train the model**\n",
    "\n",
    "Hint: Use `from pyspark.ml.classification import LogisticRegression` to  build a model by fitting your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform prediction using the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                          text|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|The last thing i ever wante...|     ham|[0.993800982052416,0.006199...|  0.0|       0.0|\n",
      "|He neva grumble but i sad l...|     ham|[0.9909979741380167,0.00900...|  0.0|       0.0|\n",
      "|Oh... Haha... Den we shld h...|     ham|[0.989072174498803,0.010927...|  0.0|       0.0|\n",
      "|THING R GOOD THANX GOT EXAM...|     ham|[0.9887591192856042,0.01124...|  0.0|       0.0|\n",
      "|\"And that is the problem. Y...|     ham|[0.988324409355555,0.011675...|  0.0|       0.0|\n",
      "|Wen ur lovable bcums angry ...|     ham|[0.9882426098634558,0.01175...|  0.0|       0.0|\n",
      "|No i'm not. I can't give yo...|     ham|[0.9881854921268145,0.01181...|  0.0|       0.0|\n",
      "|You are always putting your...|     ham|[0.9879443215569211,0.01205...|  0.0|       0.0|\n",
      "|Heart is empty without love...|     ham|[0.9879331628639672,0.01206...|  0.0|       0.0|\n",
      "|Havent planning to buy late...|     ham|[0.9878657603477324,0.01213...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform evaluations using MulticlassClassificationEvaluator**\n",
    "\n",
    "Hint: Use MulticlassClassificationEvaluator\n",
    "`from pyspark.ml.evaluation import MulticlassClassificationEvaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9494949494949494"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using TF-IDF Features\n",
    "\n",
    "**Task 8 - Now, Apply Logistic Regression using TF-IDF to see if we can improve the accuracy further**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Add HashingTF and IDF to transformation\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "# Redo Pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform predictions on Testdata using this model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                          text|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|THING R GOOD THANX GOT EXAM...|     ham|[0.9894476412953456,0.01055...|  0.0|       0.0|\n",
      "|He neva grumble but i sad l...|     ham|[0.9887095930738091,0.01129...|  0.0|       0.0|\n",
      "|Although i told u dat i'm i...|     ham|[0.9877546446366153,0.01224...|  0.0|       0.0|\n",
      "|Wen ur lovable bcums angry ...|     ham|[0.9873871799731352,0.01261...|  0.0|       0.0|\n",
      "|\"And that is the problem. Y...|     ham|[0.9870440089341924,0.01295...|  0.0|       0.0|\n",
      "|U say leh... Of course noth...|     ham|[0.9870247728278815,0.01297...|  0.0|       0.0|\n",
      "|Honeybee Said: *I'm d Sweet...|     ham|[0.9865047862576719,0.01349...|  0.0|       0.0|\n",
      "|Cos i was out shopping wif ...|     ham|[0.986161453417046,0.013838...|  0.0|       0.0|\n",
      "|Hi neva worry bout da truth...|     ham|[0.9861233055662262,0.01387...|  0.0|       0.0|\n",
      "|Yar he quite clever but aft...|     ham|[0.9857146230961832,0.01428...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate your model using MulticlassClassificationEvaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = MulticlassClassificationEvaluator(predictionCol=\"\")\n",
    "# evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give a custom message to your trained model and check results. Remember, feed your text in form of dataframe to the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of how to create a spark dataframe\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "\n",
    "print(type(schemaPeople))\n",
    "#  pyspark.sql.dataframe.DataFrame\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "ll = [('Hurry up! Answer simple questions and WINNER will get $900 prize reward! To claim call us. Valid 12 hours only.'),('Hey, How are you? Long time no see')]\n",
    "rdds = sc.parallelize(ll)\n",
    "tx = rdds.map(lambda x: Row(text=x))\n",
    "schematxt = sqlContext.createDataFrame(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Hurry up! Answer ...|\n",
      "|Hey, How are you?...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schematxt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating features for test sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|            filtered|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Hurry up! Answer ...|[hurry, up, answe...|[hurry, answer, s...|(10000,[1,721,727...|(10000,[1,721,727...|\n",
      "|Hey, How are you?...|[hey, how, are, y...|[hey, long, time,...|(10000,[7515,8157...|(10000,[7515,8157...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_new_dataset = pipelineFit.transform(schematxt)\n",
    "test_new_dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on calculated features of test sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lrModel.transform(test_new_dataset)\n",
    "# test_pred.filter(test_pred['prediction'] == 0) \\\n",
    "#     .select(\"text\",\"probability\",\"prediction\") \\\n",
    "#     .orderBy(\"probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|                text|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|Hurry up! Answer ...|[0.40530136808716...|       1.0|\n",
      "|Hey, How are you?...|[0.96833775344934...|       0.0|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred.select(\"text\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9 - Build Model using Naive Bayes algorithm\n",
    "\n",
    "### Predict on sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9731847068395234"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction on sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|                text|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|Hurry up! Answer ...|[0.40530136808716...|       1.0|\n",
      "|Hey, How are you?...|[0.96833775344934...|       0.0|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred.select(\"text\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see Naive Bayes performed well and it's able to identify message as spam! Congrats!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
