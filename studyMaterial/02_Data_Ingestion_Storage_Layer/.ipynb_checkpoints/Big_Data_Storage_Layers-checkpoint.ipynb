{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Storage Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building up any big data pipeline, very first task is to decide, **where your data will be stored?**.\n",
    "an important point to consider is to keep data in the right place based on usage. An ideal data storage option is the one where data security rules can be applied, can scale as much as required, cost efficient, easy to maintain and Faster response times.\n",
    "\n",
    "We have relational Databases that were a successful place to store our data over the years. But now we have different types of data coming into our applications from different sources, so look for a storage solution that covers all the above mentioned points. Keeping in mind the role of a Data Scientist, I'll here introduce three popular Big Data Storage tools at a very basic level:\n",
    "\n",
    "\n",
    "## 1. HDFS : Hadoop Distributed File System\n",
    "\n",
    "We have introduced HDFS while explaining the Big Data Programming models, let's have a look into how read and write operations happen in the HDFS.\n",
    "\n",
    "**Name node** - Manages the file system metadata. Metadata includes administrative information about creation time, access properties, and so on related to the data stored.\n",
    "\n",
    "**Data node** - Manage data storage on individual compute nodes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**-->  Read operation:**\n",
    "\n",
    "    - HDFS client provides command line interface to communicate with that distributed file system. So, there is no need to write any code in program languages to access data. \n",
    "    \n",
    "    - First, you request name node to get information about file blocks locations. These blocks are distributed over different machines, but all of this complexity is hidden behind HDFS API.\n",
    "    \n",
    "    - The data is served from the closest machine. Closeness here intend to  physical distance and unpredictable system load across the cluster. \n",
    "        - distance(/d1/r1/n1, /d1/r1/n1) = 0 (processes on the same node)\n",
    "        - distance(/d1/r1/n1, /d1/r1/n2) = 2 (different nodes on the same rack)\n",
    "        - distance(/d1/r1/n1, /d1/r2/n3) = 4 (nodes on different racks in the same data center)\n",
    "        - distance(/d1/r1/n1, /d2/r3/n4) = 6 (nodes in different data centers)\n",
    "    \n",
    "<img src=\"images/hdfs_distance.png\">\n",
    "\n",
    "\n",
    "**-->  Write operation:**\n",
    "\n",
    "    -  When you write a block of data into HDFS, Hadoop distributes replicas over the storage. The first replica is usually located on the same node if you write data from a DataNode machine. Otherwise, the first DataNode to put replica is chosen by random.\n",
    "    \n",
    "    - The second replica is usually placed in a different rack. You find if this racks goes down, for instance because of power supply problems, then you will still be able to access data from another rack. \n",
    "    \n",
    "    - The third replica is located on a different machine in the same rack as the second replica. Further replicas applies on the random nodes in the cluster although the system tries to avoid placing too many replicas on the same rack. \n",
    "    \n",
    "    - Same is depicted in diagram below where there is one NameNode, and multiple DataNodes (servers). b1, b2,  indicates data blocks.\n",
    "    \n",
    "<img src = \"images/hdfs.png\"> \n",
    "\n",
    "\n",
    "So this is was a high level view on uderstanding read and write operations inside hdfs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GlusterFS: Dependable Distributed File System\n",
    "\n",
    "Good storage solution must provide elasticity in both storage and performance without affecting active operations.\n",
    "\n",
    "- Scale-out storage systems based on GlusterFS are suitable for unstructured data such as documents, images, audio and video files, and log files. GlusterFS is a Open Source scalable network filesystem.\n",
    "\n",
    "- Using this, we can create large, distributed storage solutions for media streaming, data analysis, and other data- and bandwidth-intensive tasks.\n",
    "\n",
    "- You can deploy GlusterFS with the help of commodity hardware servers.\n",
    "\n",
    "- Linear scaling of performance and storage capacity.\n",
    "\n",
    "- Scale storage size up to several petabytes, which can be accessed by thousands for servers.\n",
    "\n",
    "- Keeping in mind the scope of this course, we will not go deep into this tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AWS S3\n",
    "\n",
    "Amazon Simple Storage Service (Amazon S3) is **object storage** with a simple web service interface to store and retrieve any amount of data from anywhere on the internet.\n",
    "\n",
    "It is designed to deliver 99.999999999% durability, and scale past trillions of objects worldwide.\n",
    "\n",
    "Customers use S3 as primary storage for cloud-native applications; as a bulk repository, or \"data lake,\" for analytics; as a target for backup & recovery and disaster recovery; and with serverless computing.\n",
    "\n",
    "\n",
    "It is scalable, just keep pushing data and the s3 bucket will keep scaling up. Data security is provided through various encryption provided with the service.\n",
    "\n",
    "Once data is stored on Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like S3 Standard - Infrequent Access and Amazon Glacier for archiving.\n",
    "\n",
    "\n",
    "### **Case Studies**\n",
    "\n",
    "<img src = \"images/s3_usecases.png\">\n",
    "\n",
    "\n",
    "**You can read more on these case studies on the link https://aws.amazon.com/s3**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
